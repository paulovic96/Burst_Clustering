{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from spectral_clustering import spectral_clustering\n",
    "import seaborn as sns\n",
    "import functions_for_plotting\n",
    "from asymmetric_laplacian_distribution import get_index_per_class, get_labels, labels_to_layout_mapping\n",
    "from sklearn.cluster import KMeans\n",
    "import training_set_split\n",
    "import seaborn as sns\n",
    "import prediction_strength\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend import Legend\n",
    "from training_set_split import get_training_folds\n",
    "import wagenaar_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering\n",
    "\n",
    "Clustering has the goal to divide data points into several meaningful groups such\n",
    "that points in the same group are similar and points in different groups are dissimilar to each other. These groups should afterwards reveal and give an intuiton of the data structure. \n",
    "Clustering belongs to the so called unsupervised-learning problems for which no underlying correct answer exists. While there is no correct answer there are still ways to define good or better and bad or worse solutions. Therefore also better or worse performing algorithms to solve this problem exist. \n",
    "Beside simple and widely used algorithms like K-Means Clustering or Gaussian Mixture Models (GMM), Spectral Clustering turned out to be one of the good performing clustering algorithms.  \n",
    "\n",
    "\n",
    "Spectral Clustering was developed and popularized by Shi & Malik (2000) and  Ng, Jordan, & Weiss (2002). It is based on spectal graph theory which studies the properties of graphs by using associated matrices like the adjacency or the graph Laplacian matrix.\n",
    "The idea behind spectral clustering is to reformulate the problem of clustering in terms of graph theory by constructing a similarity graphs and group the graph into subgraphs that are most similar within and dissimilar to each other.  \n",
    "In order to achieve this the algorthim must come up with a partition *'such that the edges between different groups have very low weights (which means that points in different clusters are dissimilar from each other) and the edges within a group have high weights (which means that points within the same cluster are similar to each other)'* (von Luxburg, 2007).  \n",
    "Therefore the first step of this approach is to calculate some notion of similarity between each data point and construct a similarity graph. Each vertex in the graph corresponds to a datapoint in the dataset. The connections between vertices represent the similarity between datapoints.   \n",
    "Having constructed the similarity graph the problem of finding similar groups can be reformulated by the so called *MinCut* problem (Mohar, 1991) in which we try to cut the graph into subgraphs by cutting as few edges as possible. However this simple approach runs into problems when it comes to outliers. Outliers in a graph are represented as nodes connected to the rest of the graph by an single edge (see Fig 1.). Cutting off those single nodes would lead to highly inbalanced splits. Therefore a better formulation for our grouping problem is the so called *RatioCut* (Hagen and Kahng, 1992) which adds the constraint of balancing the resulting parts in terms of number of vertices in each subgraph.  \n",
    "Wagner & Wagner however showed that minimizing the *RatioCut* problem falls into the category of NP-hard problems (Wagner & Wagner, 1993). Nevertheless a relaxed version of the *RatioCut* was shown to be solveable by the usage of the so called *Graph-Laplacian* and the *Rayleigh-Ritz theorem*. Mohar (1997) stated the eigenvectors corresponding to the smallest eigenvalues of a graph Laplacian can be related to the number of connected components in the graph.  \n",
    "\n",
    "The Graph Lablacian is defined by:  $L = D - W$ with $D = diag(d_1,...d_n)$ beeing the diagonal degree matrix of weighted degrees $d_i = \\sum_{j=1}^{n}w_{ij}$ for each node and $W$ being the weight matrix of the similarity graph.  \n",
    "\n",
    "The Cluster structure of the data can be preserved by projecting the data into a lower dimensional space using the first $k$ eigenvectors. This results in the so called *spectral-embedding* in which points of corresponding connected components are well enough embedded to be easily clustered by a simple clustering algorithm e.g. K-Means. \n",
    "Spectral clustering therefore performs dimensionality reduction using the eigenvectors of a graph laplacian  before clustering the data in the transformed space.\n",
    "Although Spectral Clustering only solves a relaxed version of the inital problem and does not have a approximation guarantee (Guattery & Miller, 1998) it shows good results in practice. The results obtained\n",
    "by spectral clustering often outperform traditional approaches. Furthermore the fact that spectral clustering formulates a standard linear algebra problem which can be efficiently solved by standard linear algebra methods makes it popular and motivated us to apply it to our data. \n",
    "\n",
    "\n",
    "## Similarity Graph\n",
    "We started the analysis with a vanilla version of the spectral clustering method. We constructed a undirected similarity graph using the pairwise euclidean distance. For each datapoint we calculated the $k=10$ nearest neighbours and connected corresponding nodes by an edge with weight 1. This led to the symmetric adjacency matrix $A$ beeing equal to the weight matrix $W$. As a next step we calculated the degree for each node and constructed the resulting graph laplacian $L$ with $L = D - A$. \n",
    "Although in some cases regularization seem to improve results especially in cases where the similarity graph contains so called k-dangling sets (Zhang and Rohe, 2018) we did not make us of it. Regularization with respect to Zhang and Rohe (Zhang and Rohe, 2018) can be applied to the weight matrix $W$ by adding a small weight to each edge  $\\tilde{W}= W + \\frac{\\tau}{n} \\cdot J$ where $J$ is the all-ones-matrix and $\\tau$ a small parameter.\n",
    "\n",
    "\n",
    "\n",
    "## Graph-Laplacian\n",
    "With respect to von Luxburg 2007 we normalized the laplacian by $L_norm = D^{-\\frac{1}{2}}(D - A)D^{-\\frac{1}{2}}$ (Ng,Jordan & Weiss,2002). While the standard laplacian is derived from the relaxed version of the *RatioCut*, the normalized follows from the relaxation of the so called *NCut* problem (Shi & Malik, 2000; Ding, 2004). Similar to the *RatioCut* the *NCut* problem includes a balancing condition. However, instead of balancing the size of each subgraph measured as its number of vertices, it is measured by the sum of weights of all edges in a subgrpah also known as the volume. The volume of a subgraph $A$ is defined as $vol(A):=\\sum_{i\\in A}d_i$ with $d_i$ beeing the degree of node $i$.\n",
    "The normalized laplacian derived from the relaxed *NCut* problem therefore not only consider the size of clusters but also its connectivity which reffering again to von Luxburg 2007 tend to give better results in practice.\n",
    "\n",
    "\n",
    "## Clustering \n",
    "We proceded with the normalized spectral clustering according to Ng, Jordan, and Weiss (Ng,Jordan & Weiss,2002). Having computed the normalized laplacian we calculated the corresponding eigenvectors and eigenvalues. Using the first k eigenvectors corresponding to the k smallest eigenvalues we constructed the matrix $U \\in \\mathbb{R}^{n\\times k}$ containing the eigenvectors $u_1,\\cdots,u_k$ as columns. We formed the matrix $T \\in \\mathbb{R}^{n\\times k}$ by normalizing the rows of $U$ to norm 1. \n",
    "Each datapoint $x_i$ is now embedded in the k dimensional space of row normalized eigenvectors as $y_i \\in \\mathbb{R}^k$. As a last step we cluster the points $y_i$ $i=1,\\cdots,n$ with the k-means algorithm into clusters $C1,\\cdots,Ck$.\n",
    "\n",
    "\n",
    "\n",
    "## Fig. 1 Min-Cut Problem\n",
    "<img src=\"mincut_problem.png\" width=\"400\" height=\"400\" align=\"left\"/>  \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "Ding, C. (2004). A tutorial on spectral clustering. Talk presented at ICML. (Slides available at\n",
    "http://crd.lbl.gov/∼cding/Spectral/)\n",
    "\n",
    "Guattery, S. and Miller, G. (1998). On the quality of spectral separators. SIAM Journal of Matrix\n",
    "Anal. Appl., 19(3), 701 – 719.\n",
    "\n",
    "Hagen, L. and Kahng, A. (1992). New spectral methods for ratio cut partitioning and clustering.\n",
    "IEEE Trans. Computer-Aided Design, 11(9), 1074 – 1085.\n",
    "\n",
    "von Luxburg U., \"A tutorial on spectral clustering\", Stat. Comput., vol. 17, no. 4, pp. 395-416, 2007.\n",
    "\n",
    "Mohar, B. (1991). The Laplacian spectrum of graphs. In Graph theory, combinatorics, and applications.\n",
    "Vol. 2 (Kalamazoo, MI, 1988) (pp. 871 – 898). New York: Wiley.\n",
    "\n",
    "Ng, A., Jordan, M., and Weiss, Y. (2002). On spectral clustering: analysis and an algorithm. In T. Dietterich, S. Becker, and Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14 (pp. 849 – 856). MIT Press.\n",
    "\n",
    "Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on Pattern\n",
    "Analysis and Machine Intelligence, 22(8), 888 – 905.\n",
    "\n",
    "Wagner, D. and Wagner, F. (1993). Between min cut and graph bisection. In Proceedings of the 18th International Symposius on Mathematical Foundations of Computer Science (MFCS) (pp. 744 – 750). London: Springer.\n",
    "\n",
    "Zhang, Y. and Rohe, K. Understanding regularized spectral clustering via graph conduc- tance. In Advances in Neural Information Processing Systems, pages 10631–10640, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "- load dataset\n",
    "- load culture dict specifiying for each culture start and end point with respect to the dataset for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/raw_data/daily_spontanous_dense/culture2_over_days/\"\n",
    "data = np.load(data_dir + \"data_burst_by_time_culture_2_2.npy\").T\n",
    "culture_dict = np.load(data_dir + \"culture_dict_culture_2_2.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral_clustering import calculate_dist_matrix, construct_knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix, sorted_dist_matrix = calculate_dist_matrix(data, metric ='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n"
     ]
    }
   ],
   "source": [
    "A = construct_knn_graph(dist_matrix,sorted_dist_matrix,'euclidean', k=10, mutual = False, weighting = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build mutual KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n"
     ]
    }
   ],
   "source": [
    "A_mutual = construct_knn_graph(dist_matrix,sorted_dist_matrix,'euclidean', k=10, mutual = True, weighting = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sk = kneighbors_graph(data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   7,  233,  246,  250,  252,  276,  350,  356,  364,  398,  562,\n",
       "         589,  598,  646,  661,  715,  958, 2029]),)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(A[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    4,   19,   23,  174,  213,  253,  355,  356,  397,  398,\n",
       "         446,  463,  484,  531,  544,  589,  627,  633,  684,  751,  902,\n",
       "        2029]),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(A[7] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7, 246, 250, 252, 356, 562, 589, 646, 715, 958]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(A_sk.toarray()[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  4,  19, 356, 463, 484, 506, 544, 589, 627, 751]),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(A_sk.toarray()[7] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"euclidean\"\n",
    "metric_type = \"distance\"\n",
    "range_clusters = range(1,101)\n",
    "k=10\n",
    "reg=None\n",
    "mutual = False\n",
    "weighting = False\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n"
     ]
    }
   ],
   "source": [
    "labels, eigvec, eigval = spectral_clustering(data, metric, metric_type, range_clusters, k=k, mutual = mutual, weighting = weighting, normalize = normalize, reg_lambda = reg, save_laplacian = False, save_eigenvalues_and_vectors = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"labels_culture_2_2_Euclidean_k=10_reg=none_100clusters\" ,labels)\n",
    "#np.save(\"eigval_culture_2_2_Euclidean_k=10_reg=none_100clusters\" ,eigval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Splitted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data_splits(data, train_fold_indices, valid_fold_indices,metric = \"euclidean\",metric_type=\"distance\", range_clusters = range(1,50),k=10, reg=None, train_only = False,mutual=False,weighting=False, normalize = True):\n",
    "    print(\"Start clustering training folds!\")\n",
    "    train_fold_labels = []\n",
    "    train_fold_eigenvalues = []\n",
    "    for fold_indices in train_fold_indices:\n",
    "        train_data = data[fold_indices]\n",
    "        labels, eigvec, eigval = spectral_clustering(train_data, metric, metric_type, range_clusters, k=k, mutual = mutual, weighting = weighting, normalize = normalize, reg_lambda = reg, save_laplacian = False, save_eigenvalues_and_vectors = False)\n",
    "        train_fold_labels.append(labels)\n",
    "        train_fold_eigenvalues.append(eigval)\n",
    "    print(\"Done...\")\n",
    "    if train_only:\n",
    "        return train_fold_labels, train_fold_eigenvalues\n",
    "    \n",
    "    else:\n",
    "        print(\"Start clustering validation folds!\")\n",
    "        valid_fold_labels = []\n",
    "        valid_fold_eigenvalues = []\n",
    "        for fold_indices in valid_fold_indices:\n",
    "            valid_data = data[fold_indices]\n",
    "            labels, eigvec, eigval = spectral_clustering(valid_data, metric, metric_type, range_clusters, k=k, mutual = mutual, weighting = weighting, normalize = normalize, reg_lambda = reg, save_laplacian = False, save_eigenvalues_and_vectors = False)\n",
    "            valid_fold_labels.append(labels)\n",
    "            valid_fold_eigenvalues.append(eigval)\n",
    "        print(\"Done...\")\n",
    "        return train_fold_labels, train_fold_eigenvalues, valid_fold_labels, valid_fold_eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Data Splitting\n",
    "split styles: \n",
    "- 'balanced' with respect to cultures (5 fold random split for each culture) --> culture_dict must be provided\n",
    "- 'random' 5 fold random split --> no culture_dict needed\n",
    "- 'unbalanced' some clusters only occure in training fold not in validation fold --> culture_dict and only_training_clusters (list of cluster per fold which should only be occur in training data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_style = \"balanced\"\n",
    "folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_indices, valid_fold_indices = training_set_split.get_training_folds(data,culture_dict,cluster_split = split_style,folds = folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"euclidean\"\n",
    "metric_type = \"distance\"\n",
    "range_clusters = range(1,101)\n",
    "k=10\n",
    "reg=None\n",
    "mutual = False\n",
    "weighting = False\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start clustering training folds!\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Done...\n",
      "Start clustering validation folds!\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Calculate euclidean matrix for constructing KNN-Graph\n",
      "Build symmetric KNN-Graph based on Distance of data points!\n",
      "Weighting: False\n",
      "Calculate Normalized Laplacians\n",
      "Normalization: symmetric\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "train_fold_labels, train_fold_eigenvalues, valid_fold_labels, valid_fold_eigenvalues = cluster_data_splits(data, train_fold_indices, valid_fold_indices,metric = metric,metric_type=metric_type,range_clusters = range_clusters,k=k, reg=reg, train_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"labels_culture_2_2_Euclidean_k=10_reg=None_5_fold_balanced_train_100clusters.npy\" ,train_fold_labels)\n",
    "#np.save(\"eigval_culture_2_2_Euclidean_k=10_reg=None_5_fold_balanced_train_100clusters.npy\" ,train_fold_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"labels_culture_2_2_Euclidean_k=10_reg=None_5_fold_balanced_valid_100clusters.npy\" ,valid_fold_labels)\n",
    "#np.save(\"eigval_culture_2_2_Euclidean_k=10_reg=None_5_fold_balanced_valid_100clusters.npy\" ,valid_fold_eigenvalues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
