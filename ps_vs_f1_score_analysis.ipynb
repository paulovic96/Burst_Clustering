{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paule/anaconda3/envs/bon17/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.nearest_centroid module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from spectral_clustering import spectral_clustering\n",
    "import functions_for_plotting\n",
    "from asymmetric_laplacian_distribution import get_index_per_class, get_labels, labels_to_layout_mapping\n",
    "from sklearn.cluster import KMeans\n",
    "import training_set_split\n",
    "import seaborn as sns\n",
    "import prediction_strength\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and True Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------- DATA ------------------------------------------------------------------\n",
    "data_dir = \"data/\"\n",
    "\n",
    "clear_data = np.load(data_dir + \"clearly_separated_data_F_signal_noise.npy\")\n",
    "ambig_data = np.load(data_dir + \"ambiguous_data_tau_amplitude_F_signal_noise.npy\")\n",
    "#np.load(data_dir + \"ambiguous_data_tau_amplitude_F_signal_noise.npy\") #np.load(data_dir + \"clearly_separated_data_F_signal_noise.npy\")\n",
    "\n",
    "clear_amplitude_conditions = [\"S\", \"M\", \"L\"]  #[\"S\", \"S/M\", \"M\", \"M/L\", \"L\"] #[\"S\", \"M\", \"L\"]\n",
    "ambig_amplitude_conditions = [\"S\", \"S/M\", \"M\", \"M/L\", \"L\"]\n",
    "\n",
    "clear_time_constant_conditions = [\"equal_sharp\", \"equal_wide\", \"wide_sharp_negative_skew\", \"sharp_wide_positive_skew\"]\n",
    "ambig_time_constant_conditions = [\"equal_sharp\", \"equal_medium\", \"equal_wide\", \"wide_sharp_negative_skew\", \"wide_medium_negative_skew\",\"medium_sharp_negative_skew\",\"sharp_wide_positive_skew\", \"medium_wide_positive_skew\" ,\"sharp_medium_positive_skew\"]\n",
    "\n",
    "#[\"equal_sharp\", \"equal_medium\", \"equal_wide\", \"wide_sharp_negative_skew\", \"wide_medium_negative_skew\",\"medium_sharp_negative_skew\",\"sharp_wide_positive_skew\", \"medium_wide_positive_skew\" ,\"sharp_medium_positive_skew\"]\n",
    "#[\"equal_sharp\", \"equal_wide\", \"wide_sharp_negative_skew\", \"sharp_wide_positive_skew\"]\n",
    "\n",
    "ambiguous_conditions = [\"S/M\", \"M/L\", \"equal_medium\", \"wide_medium_negative_skew\", \"medium_sharp_negative_skew\", \"medium_wide_positive_skew\", \"sharp_medium_positive_skew\"]\n",
    "\n",
    "samples_per_condition = 1000\n",
    "samples_per_ambiguous_condition = 400\n",
    "\n",
    "ambig_cluster_dict = get_index_per_class(ambig_amplitude_conditions,ambig_time_constant_conditions, ambiguous_conditions, samples_per_condition, samples_per_ambiguous_condition)\n",
    "clear_cluster_dict = get_index_per_class(clear_amplitude_conditions,clear_time_constant_conditions, [], samples_per_condition, samples_per_ambiguous_condition)\n",
    "\n",
    "\n",
    "clear_true_labels = get_labels(clear_data, clear_cluster_dict)\n",
    "ambig_true_labels = get_labels(ambig_data, ambig_cluster_dict)\n",
    "\n",
    "clear_clusters_ordered = list(range(0,len(clear_cluster_dict)+1))\n",
    "clear_layout_label_mapping = labels_to_layout_mapping(clear_clusters_ordered, 4, (1,4)) #labels_to_layout_mapping(clusters_ordered, 4, (1,4)) #labels_to_layout_mapping(clusters_ordered, 9, (2,5))\n",
    "\n",
    "ambig_clusters_ordered = list(range(0,len(ambig_cluster_dict)+1))\n",
    "ambig_layout_label_mapping = labels_to_layout_mapping(ambig_clusters_ordered, 9, (2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Balanced Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_train_fold_indices, _ = training_set_split.get_training_folds(clear_data, clear_cluster_dict,cluster_split=\"balanced\",folds = 2)\n",
    "ambig_train_fold_indices, _ = training_set_split.get_training_folds(ambig_data, ambig_cluster_dict,cluster_split=\"balanced\",folds = 2)\n",
    "\n",
    "\n",
    "clear_training_set = clear_data[clear_train_fold_indices[0]]\n",
    "clear_validation_set = clear_data[clear_train_fold_indices[1]]\n",
    "\n",
    "ambig_training_set = ambig_data[ambig_train_fold_indices[0]]\n",
    "ambig_validation_set = ambig_data[ambig_train_fold_indices[1]]\n",
    "\n",
    "clear_true_labels_training = clear_true_labels[clear_train_fold_indices[0]]\n",
    "clear_true_labels_validation = clear_true_labels[clear_train_fold_indices[1]]\n",
    "\n",
    "ambig_true_labels_training = ambig_true_labels[ambig_train_fold_indices[0]]\n",
    "ambig_true_labels_validation = ambig_true_labels[ambig_train_fold_indices[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Unbalanced Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(42)\n",
    "clear_clusters = np.arange(12)\n",
    "ambig_clusters = np.arange(45)\n",
    "np.random.shuffle(clear_clusters)\n",
    "np.random.shuffle(ambig_clusters)\n",
    "\n",
    "clear_training_clusters = clear_clusters[0:6]\n",
    "ambig_training_clusters = ambig_clusters[0:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_train_fold_indices, clear_valid_fold_indices = training_set_split.get_training_folds(clear_data, clear_cluster_dict,cluster_split=\"unbalanced\",training_clusters = clear_training_clusters,folds = 2)\n",
    "ambig_train_fold_indices, ambig_valid_fold_indices = training_set_split.get_training_folds(ambig_data, ambig_cluster_dict,cluster_split=\"unbalanced\",training_clusters = ambig_training_clusters, folds = 2)\n",
    "\n",
    "\n",
    "clear_training_set = clear_data[clear_train_fold_indices]\n",
    "clear_validation_set = clear_data[clear_valid_fold_indices]\n",
    "\n",
    "ambig_training_set = ambig_data[ambig_train_fold_indices]\n",
    "ambig_validation_set = ambig_data[ambig_train_fold_indices]\n",
    "\n",
    "clear_true_labels_training = clear_true_labels[clear_train_fold_indices]\n",
    "clear_true_labels_validation = clear_true_labels[clear_valid_fold_indices]\n",
    "\n",
    "ambig_true_labels_training = ambig_true_labels[ambig_train_fold_indices]\n",
    "ambig_true_labels_validation = ambig_true_labels[ambig_valid_fold_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(clear_true_labels_training,return_counts = True))\n",
    "print(np.unique(clear_true_labels_validation,return_counts = True))\n",
    "print(np.unique(ambig_true_labels_training,return_counts = True))\n",
    "print(np.unique(ambig_true_labels_validation,return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = 10\n",
    "#reg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels, eigvec, eigval = spectral_clustering(clear_training_set, \"euclidean\", range(1,50),  k=k, mutual = False, weighting = \"distance\", normalize = True, reg_lambda = reg, save_laplacian = False, save_eigenvalues_and_vectors = False)\n",
    "#np.save(\"labels_k=%d_reg=%s_clear_training_unbalanced\" % (k, str(reg)),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels, eigvec, eigval = spectral_clustering(clear_validation_set, \"euclidean\", range(1,50),  k=k, mutual = False, weighting = \"distance\", normalize = True, reg_lambda = reg, save_laplacian = False, save_eigenvalues_and_vectors = False)\n",
    "#np.save(\"labels_k=%d_reg=%s_clear_validation_unbalanced\" % (k, str(reg)),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_file_clusters = \"true_clusters_clear_unbalanced_validation_mean.pdf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions_for_plotting.plot_clusters(clear_validation_set, clear_true_labels_validation,clear_true_labels_validation, 3,4, clear_layout_label_mapping,figsize=(20,20),n_bursts = 100,y_lim = (0,16),save_file=save_file_clusters ,subplot_adjustments= [0.05,0.95,0.03,0.9,0.4, 0.15], plot_mean=True, title= \"Validation Set Clusters (Mean)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction strength vs. F1 score \n",
    "- True positives: same cluster in  training, same cluster in validation\n",
    "- False positives: different cluster in training, same cluster in validation\n",
    "- True negatives: different cluster in training, different cluster in validation\n",
    "- False negatives: same cluster in training, different cluster in validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_strength import calculate_centroids_and_predict_validation_data\n",
    "from prediction_strength import get_confusion_matrix\n",
    "from prediction_strength import get_recall\n",
    "from prediction_strength import get_precision\n",
    "from prediction_strength import get_F1_score\n",
    "from math import factorial\n",
    "from itertools import permutations\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1_score_per_k(data, train_indices, valid_indices, train_labels, valid_labels,combination_type = \"full\", true_train_labels = None, own_combinations=None):\n",
    "    k_clusters = list(valid_labels.keys())\n",
    "    training_set = data[train_indices]\n",
    "    validation_set = data[valid_indices]\n",
    "    \n",
    "    if combination_type == \"true\":\n",
    "        print(\"Calculate F1 score based on true training centroids!\")\n",
    "        if not true_train_labels is None:\n",
    "            true_F1_score_per_k = {} \n",
    "            for k in k_clusters:\n",
    "                train_labels_k = true_train_labels\n",
    "                valid_labels_k = valid_labels[k]\n",
    "                \n",
    "                if len(np.unique(train_labels_k))>1:\n",
    "                    centroids_k, labels_centroids_based = calculate_centroids_and_predict_validation_data(training_set,train_labels_k,validation_set)\n",
    "\n",
    "                else:\n",
    "                    labels_centroids_based = np.zeros(len(validation_set))\n",
    "\n",
    "                unique_valid_labels = list(range(k))\n",
    "                unique_centroid_labels = list(np.unique(train_labels_k))\n",
    "\n",
    "                true_positives, false_positives, true_negatives, false_negatives = get_confusion_matrix(valid_labels_k,labels_centroids_based,unique_valid_labels, unique_centroid_labels)\n",
    "                precision = get_precision(true_positives,false_positives)\n",
    "                recall = get_recall(true_positives, false_negatives)\n",
    "                f1 = get_F1_score(recall, precision)\n",
    "\n",
    "                true_F1_score_per_k[k] = f1 \n",
    "\n",
    "            return true_F1_score_per_k\n",
    "        else:\n",
    "            print(\"True labels for the training set not found..Please provide true labels!!\")\n",
    "    \n",
    "    elif combination_type == \"equal\":\n",
    "        print(\"Calculate F1 score based on training centroids assuming same number of clusters in both sets!\")\n",
    "        F1_score_per_k = {}\n",
    "        for k in k_clusters:\n",
    "            train_labels_k = train_labels[k]\n",
    "            valid_labels_k = valid_labels[k]\n",
    "\n",
    "            if k>1:\n",
    "                centroids_k, labels_centroids_based = calculate_centroids_and_predict_validation_data(training_set,train_labels_k,validation_set)\n",
    "\n",
    "            else:\n",
    "                labels_centroids_based = np.zeros(len(validation_set))\n",
    "\n",
    "            unique_valid_labels = list(range(k))\n",
    "            unique_centroid_labels = list(range(k))\n",
    "\n",
    "            true_positives, false_positives, true_negatives, false_negatives = get_confusion_matrix(valid_labels_k,labels_centroids_based,unique_valid_labels, unique_centroid_labels)\n",
    "            precision = get_precision(true_positives,false_positives)\n",
    "            recall = get_recall(true_positives, false_negatives)\n",
    "            f1 = get_F1_score(recall, precision)\n",
    "\n",
    "            F1_score_per_k[k] = f1 \n",
    "\n",
    "        return F1_score_per_k\n",
    "        \n",
    "        \n",
    "    elif combination_type == \"full\":\n",
    "        print(\"Calculate F1 score based on training centroids with full permutation of possible clusters in both sets!\")\n",
    "        F1_score_per_k_combination = {} \n",
    "        counter = 0\n",
    "        for k1k2 in product(k_clusters, repeat=2):\n",
    "            counter += 1\n",
    "            if counter%50 == 0:\n",
    "                print(\"Step:%d\" % counter)\n",
    "            train_labels_k = train_labels[k1k2[0]]\n",
    "            valid_labels_k = valid_labels[k1k2[1]]\n",
    "\n",
    "            if k1k2[0] >1:\n",
    "                centroids_k, labels_centroids_based = calculate_centroids_and_predict_validation_data(training_set,train_labels_k,validation_set)\n",
    "\n",
    "            else:\n",
    "                labels_centroids_based = np.zeros(len(validation_set))\n",
    "\n",
    "            unique_valid_labels = list(range(k1k2[1]))\n",
    "            unique_centroid_labels = list(range(k1k2[0]))\n",
    "\n",
    "            true_positives, false_positives, true_negatives, false_negatives = get_confusion_matrix(valid_labels_k,labels_centroids_based,unique_valid_labels, unique_centroid_labels)\n",
    "            precision = get_precision(true_positives,false_positives)\n",
    "            recall = get_recall(true_positives, false_negatives)\n",
    "            f1 = get_F1_score(recall, precision)\n",
    "\n",
    "            F1_score_per_k_combination[k1k2] = f1\n",
    "\n",
    "        return F1_score_per_k_combination\n",
    "    \n",
    "    elif combination_type == \"own\":\n",
    "        print(\"Calculate F1 score based on training centroids with provided combination of possible clusters in both sets!\")\n",
    "        F1_score_per_k_combination = {} \n",
    "        for k1k2 in own_combinations:\n",
    "            train_labels_k = train_labels[k1k2[0]]\n",
    "            valid_labels_k = valid_labels[k1k2[1]]\n",
    "\n",
    "            if k1k2[0] >1:\n",
    "                centroids_k, labels_centroids_based = calculate_centroids_and_predict_validation_data(training_set,train_labels_k,validation_set)\n",
    "\n",
    "            else:\n",
    "                labels_centroids_based = np.zeros(len(validation_set))\n",
    "\n",
    "            unique_valid_labels = list(range(k1k2[1]))\n",
    "            unique_centroid_labels = list(range(k1k2[0]))\n",
    "\n",
    "            true_positives, false_positives, true_negatives, false_negatives = get_confusion_matrix(valid_labels_k,labels_centroids_based,unique_valid_labels, unique_centroid_labels)\n",
    "            precision = get_precision(true_positives,false_positives)\n",
    "            recall = get_recall(true_positives, false_negatives)\n",
    "            f1 = get_F1_score(recall, precision)\n",
    "\n",
    "            F1_score_per_k_combination[k1k2] = f1\n",
    "\n",
    "        return F1_score_per_k_combination       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "reg = 0.01\n",
    "\n",
    "clear_prediction_strength_dir = \"Toy_data/Clearly_Separated/Prediction_Strength/\"\n",
    "ambig_prediction_strength_dir = \"Toy_data/Ambiguous/Ambiguous_Tau_Amplitude/Prediction_Strength/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels\n",
    "### balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear_training_set_labels = np.load(clear_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "#clear_validation_set_labels = np.load(clear_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "ambig_training_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "ambig_validation_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "#clear_train_labels = {}\n",
    "#clear_valid_labels = {}\n",
    "#for i, labels in enumerate(clear_training_set_labels):\n",
    "#    clear_train_labels[i+1] = labels\n",
    "#    clear_valid_labels[i+1] = clear_validation_set_labels[i]\n",
    "    \n",
    "ambig_train_labels = {}\n",
    "ambig_valid_labels = {}\n",
    "for i, labels in enumerate(ambig_training_set_labels):\n",
    "    ambig_train_labels[i+1] = labels\n",
    "    ambig_valid_labels[i+1] = ambig_validation_set_labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_training_set_labels = np.load(\"labels_k=%d_reg=%s_clear_training_unbalanced.npy\" % (k, str(reg)))\n",
    "clear_validation_set_labels = np.load(\"labels_k=%d_reg=%s_clear_validation_unbalanced.npy\" % (k, str(reg)))\n",
    "\n",
    "#ambig_training_set_labels = np.load(\"labels_k=%d_reg=%s_ambig_training_unbalanced.npy\" % (k, str(reg)))\n",
    "#ambig_validation_set_labels = np.load(\"labels_k=%d_reg=%s_ambig_validation_unbalanced.npy\" % (k, str(reg)))\n",
    "\n",
    "clear_train_labels = {}\n",
    "clear_valid_labels = {}\n",
    "for i, labels in enumerate(clear_training_set_labels):\n",
    "    clear_train_labels[i+1] = labels\n",
    "    clear_valid_labels[i+1] = clear_validation_set_labels[i]\n",
    "    \n",
    "#ambig_train_labels = {}\n",
    "#ambig_valid_labels = {}\n",
    "#for i, labels in enumerate(ambig_training_set_labels):\n",
    "#    ambig_train_labels[i+1] = labels\n",
    "#    ambig_valid_labels[i+1] = ambig_validation_set_labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score & Prediction Strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "clear_F1_score_per_k = get_F1_score_per_k(clear_data, clear_train_fold_indices[0], clear_train_fold_indices[1], clear_train_labels, clear_valid_labels, combination_type = \"true\" ,true_train_labels = clear_true_labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_prediction_strengths_per_k,_ = prediction_strength.get_prediction_strength_per_k(clear_data, clear_train_fold_indices[0], clear_train_fold_indices[1], clear_train_labels, clear_valid_labels, per_sample = False, true_train_labels = clear_true_labels_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "ambig_F1_score_per_k = get_F1_score_per_k(ambig_data, ambig_train_fold_indices[0], ambig_train_fold_indices[1], ambig_train_labels, ambig_valid_labels,combination_type = \"true\" ,true_train_labels = ambig_true_labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_prediction_strengths_per_k,_ = prediction_strength.get_prediction_strength_per_k(ambig_data, ambig_train_fold_indices[0], ambig_train_fold_indices[1], ambig_train_labels, ambig_valid_labels, per_sample = False, true_train_labels = ambig_true_labels_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Clustres in Ambiguous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_from_ambig_dataset, counts = np.unique(ambig_true_labels, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_clusters_from_ambig = clusters_from_ambig_dataset[np.where(counts!= 400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_clusters_from_ambig_idx_validation = np.where(np.isin(ambig_true_labels_validation,clear_clusters_from_ambig) == True)[0]\n",
    "clear_clusters_from_ambig_idx_training = np.where(np.isin(ambig_true_labels_training,clear_clusters_from_ambig) == True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_clear_train_inidices = ambig_train_fold_indices[0][clear_clusters_from_ambig_idx_training]\n",
    "ambig_clear_valid_inidices = ambig_train_fold_indices[1][clear_clusters_from_ambig_idx_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_clear_valid_labels = {}\n",
    "for i, labels in enumerate(ambig_validation_set_labels):\n",
    "    ambig_clear_valid_labels[i+1] = labels[clear_clusters_from_ambig_idx_validation]\n",
    "\n",
    "ambig_clear_true_train_labels = ambig_true_labels_training[clear_clusters_from_ambig_idx_training]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "ambig_F1_score_per_k_clear_clusters = get_F1_score_per_k(ambig_data, ambig_clear_train_inidices, ambig_clear_valid_inidices, None, ambig_clear_valid_labels,combination_type = \"true\" ,true_train_labels = ambig_clear_true_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prediction_strength' from '/Users/paule/Desktop/Burst_Clustering/prediction_strength.py'>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(prediction_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_prediction_strengths_per_k,_ = prediction_strength.get_prediction_strength_per_k(ambig_data, ambig_clear_train_inidices, ambig_clear_valid_inidices, None, ambig_clear_valid_labels, per_sample = False, true_train_labels = ambig_clear_true_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of clear clusters for all regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "regs = [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]\n",
    "k = 10\n",
    "for reg in regs:\n",
    "    #clear_training_set_labels = np.load(clear_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "    #clear_validation_set_labels = np.load(clear_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "    ambig_training_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "    ambig_validation_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "    #clear_train_labels = {}\n",
    "    #clear_valid_labels = {}\n",
    "    #for i, labels in enumerate(clear_training_set_labels):\n",
    "    #    clear_train_labels[i+1] = labels\n",
    "    #    clear_valid_labels[i+1] = clear_validation_set_labels[i]\n",
    "\n",
    "    ambig_train_labels = {}\n",
    "    ambig_valid_labels = {}\n",
    "    for i, labels in enumerate(ambig_training_set_labels):\n",
    "        ambig_train_labels[i+1] = labels\n",
    "        ambig_valid_labels[i+1] = ambig_validation_set_labels[i]\n",
    "        \n",
    "    clusters_from_ambig_dataset, counts = np.unique(ambig_true_labels, return_counts = True)\n",
    "    clear_clusters_from_ambig = clusters_from_ambig_dataset[np.where(counts!= 400)]\n",
    "    clear_clusters_from_ambig_idx_validation = np.where(np.isin(ambig_true_labels_validation,clear_clusters_from_ambig) == True)[0]\n",
    "    clear_clusters_from_ambig_idx_training = np.where(np.isin(ambig_true_labels_training,clear_clusters_from_ambig) == True)[0]\n",
    "    \n",
    "    ambig_clear_train_inidices = ambig_train_fold_indices[0][clear_clusters_from_ambig_idx_training]\n",
    "    ambig_clear_valid_inidices = ambig_train_fold_indices[1][clear_clusters_from_ambig_idx_validation]\n",
    "    \n",
    "    ambig_clear_valid_labels = {}\n",
    "    for i, labels in enumerate(ambig_validation_set_labels):\n",
    "        ambig_clear_valid_labels[i+1] = labels[clear_clusters_from_ambig_idx_validation]\n",
    "\n",
    "    ambig_clear_true_train_labels = ambig_true_labels_training[clear_clusters_from_ambig_idx_training]    \n",
    "    ambig_F1_score_per_k_clear_clusters = get_F1_score_per_k(ambig_data, ambig_clear_train_inidices, ambig_clear_valid_inidices, None, ambig_clear_valid_labels,combination_type = \"true\" ,true_train_labels = ambig_clear_true_train_labels)\n",
    "    np.save(\"F1_clear_clusters_k=%d_reg=%s_ambig_balanced_true\" % (k,str(reg)),ambig_F1_score_per_k_clear_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Mean Prediction Strength vs. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20,10)\n",
    "plot_adjustments = [0.05,0.08,0.95, 0.91]\n",
    "k=10\n",
    "configuration = \"k=%d - reg=%s\" % (k,str(reg))\n",
    "#save_file = \"PS_vs_F1_k=%d_reg=%s_ambig_balanced_true.pdf\" % (k,str(reg))\n",
    "save_file = \"F1_clear_clusters_k=%d_reg=%s_ambig_balanced_true.pdf\" % (k,str(reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_strengths_per_k = ambig_prediction_strengths_per_k\n",
    "F1_score_per_k = ambig_F1_score_per_k\n",
    "F1_score_per_k_clear_clusters = ambig_F1_score_per_k_clear_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "k_clusters = list(F1_score_per_k.keys())\n",
    "\n",
    "mean_prediction_strengths = []\n",
    "err_prediction_strengths = []\n",
    "min_prediction_strengths = []\n",
    "F1_scores = []\n",
    "F1_scores_clear_clusters = []\n",
    "\n",
    "for k in k_clusters:\n",
    "    #mean_prediction_strengths.append(np.mean(prediction_strengths_per_k[k]))\n",
    "    #err_prediction_strengths.append(np.std(prediction_strengths_per_k[k]))\n",
    "    #min_prediction_strengths.append(np.amin(prediction_strengths_per_k[k]))\n",
    "    F1_scores.append(F1_score_per_k[k])\n",
    "    F1_scores_clear_clusters.append(F1_score_per_k_clear_clusters[k])\n",
    "\n",
    "\n",
    "#upper_err = np.asarray(err_prediction_strengths) - np.maximum(0,(np.asarray(err_prediction_strengths)+np.asarray(mean_prediction_strengths)-1))\n",
    "#lower_err = np.asarray(err_prediction_strengths)\n",
    "#err = np.stack((lower_err,upper_err), axis=0)\n",
    "\n",
    "ax.plot(k_clusters, F1_scores, \"o-\", label=\"F1-Scores\",color = \"C0\",linewidth=3)\n",
    "ax.plot(k_clusters, F1_scores_clear_clusters, \"o-\", label=\"F1-Scores (Clear Clusters)\",color = \"C03\",linewidth=3)\n",
    "#ax.plot(k_clusters, mean_prediction_strengths, \"o-\",label=\"Mean PS\",color = \"C0\",linewidth=3)\n",
    "#ax.plot(k_clusters, min_prediction_strengths, \"o-\", color = \"C01\", label=\"Min PS\",linewidth=3)\n",
    "\n",
    "argmax_f1 = np.argmax(F1_scores[1:]) + 1\n",
    "argmax_f1_clear_clusters = np.argmax(F1_scores_clear_clusters[1:]) + 1\n",
    "#argmax_mean_ps = np.argmax(mean_prediction_strengths[1:]) +1\n",
    "#argmax_min_ps = np.argmax(min_prediction_strengths[1:])+1\n",
    "\n",
    "\n",
    "#if np.abs(argmax_mean_ps-argmax_f1) <8:\n",
    "if np.abs(argmax_f1_clear_clusters-argmax_f1) <8:\n",
    "    #if F1_scores[argmax_f1] <= mean_prediction_strengths[argmax_mean_ps]:\n",
    "    if F1_scores[argmax_f1] <= F1_scores_clear_clusters[argmax_f1_clear_clusters]:\n",
    "        ps_shift = 0.03\n",
    "        f1_shift = 0\n",
    "    else:\n",
    "        ps_shift = 0\n",
    "        f1_shift = 0.03\n",
    "else:\n",
    "    ps_shift = 0\n",
    "    f1_shift = 0\n",
    "\n",
    "#ax.annotate(\"#%d|Score=%.3f\" % (argmax_mean_ps+1, mean_prediction_strengths[argmax_mean_ps]), (k_clusters[argmax_mean_ps] - 1, mean_prediction_strengths[argmax_mean_ps] + 0.03 + ps_shift),fontsize = 16, color = \"C0\")\n",
    "ax.annotate(\"#%d|Score=%.3f\" % (argmax_f1+1, F1_scores[argmax_f1]), (k_clusters[argmax_f1] - 1, F1_scores[argmax_f1] + 0.03 + f1_shift), fontsize=16, color = \"C0\")\n",
    "ax.annotate(\"#%d|Score=%.3f\" % (argmax_f1_clear_clusters+1, F1_scores_clear_clusters[argmax_f1_clear_clusters]), (k_clusters[argmax_f1_clear_clusters] - 1, F1_scores_clear_clusters[argmax_f1_clear_clusters] + 0.03 + ps_shift),fontsize = 16, color = \"C03\")\n",
    "#ax.annotate(\"#%d|Score=%.3f\" % (argmax_min_ps+1, min_prediction_strengths[argmax_min_ps]), (k_clusters[argmax_min_ps] - 1, min_prediction_strengths[argmax_min_ps] + 0.03), fontsize=16, color = \"C01\")\n",
    "\n",
    "#title = \"Prediction Strength vs. F1-Score for Clustering with k Clusters \\n\" + configuration \n",
    "title = \"F1-Score for Clustering with k Clusters \\n\" + configuration \n",
    "\n",
    "ax.set_title(title, fontsize=22, pad=20)\n",
    "ax.set_xticks(k_clusters)\n",
    "ax.set_xlabel(\"# Number of clusters\", fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"Score\", fontsize=18, labelpad=10),\n",
    "ax.set_ylim((0, 1.1))\n",
    "ax.tick_params(axis='y',labelsize=14)\n",
    "ax.tick_params(axis='x',labelsize=14)\n",
    "\n",
    "ax.set_yticks(np.arange(0, 1.1,0.1))\n",
    "left = plot_adjustments[0]\n",
    "bottom = plot_adjustments[1]\n",
    "right = plot_adjustments[2]\n",
    "top = plot_adjustments[3]\n",
    "\n",
    "plt.subplots_adjust(left,bottom,right, top)\n",
    "\n",
    "ax.legend(fontsize = 14, loc=\"lower right\")\n",
    "\n",
    "plt.savefig(save_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot F1 score for different regularizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20,10)\n",
    "plot_adjustments = [0.05,0.08,0.95, 0.91]\n",
    "configuration = \"k=%d - reg=%s\" % (k,str(reg))\n",
    "save_file = \"F1_regularization_comparison_0.01_to_0.1_k=10.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "regs = regs = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "configuration = \"SC-Configuration: k=%d\" % (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_scores_clear_clusters_reg = {}\n",
    "for reg in regs: \n",
    "    f1_dict = np.load(\"F1_clear_clusters_k=%d_reg=%s_ambig_balanced_true.npy\" % (k,str(reg)),allow_pickle=True).item()\n",
    "    k_clusters = list(f1_dict.keys())\n",
    "    F1_scores = []\n",
    "    for i in k_clusters:\n",
    "        #mean_prediction_strengths.append(np.mean(prediction_strengths_per_k[k]))\n",
    "        #err_prediction_strengths.append(np.std(prediction_strengths_per_k[k]))\n",
    "        #min_prediction_strengths.append(np.amin(prediction_strengths_per_k[k]))\n",
    "        F1_scores.append(f1_dict[i])\n",
    "    F1_scores_clear_clusters_reg[reg] = F1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "k_clusters = range(1,50)\n",
    "for i,reg in enumerate(regs):\n",
    "    F1_scores = F1_scores_clear_clusters_reg[reg]\n",
    "    ax.plot(k_clusters, F1_scores, \"o-\", label=\"reg=%s\" % str(reg),color = \"C0%d\" % i,linewidth=3)\n",
    "\n",
    "    argmax_f1 = np.argmax(F1_scores[1:]) + 1\n",
    "\n",
    "    ax.annotate(\"#%d|Score=%.3f\" % (argmax_f1+1, F1_scores[argmax_f1]), (k_clusters[argmax_f1] - 1, F1_scores[argmax_f1] + 0.03 - i*0.03), fontsize=16, color = \"C0%d\" % i)\n",
    "\n",
    "\n",
    "title = \"F1-Score of Clear Clusters for Clustering with k Clusters \\n\" + configuration \n",
    "\n",
    "ax.set_title(title, fontsize=22, pad=20)\n",
    "ax.set_xticks(k_clusters)\n",
    "ax.set_xlabel(\"# Number of clusters\", fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"F1-Score\", fontsize=18, labelpad=10),\n",
    "ax.set_ylim((0, 1.1))\n",
    "ax.tick_params(axis='y',labelsize=14)\n",
    "ax.tick_params(axis='x',labelsize=14)\n",
    "\n",
    "ax.set_yticks(np.arange(0, 1.1,0.1))\n",
    "left = plot_adjustments[0]\n",
    "bottom = plot_adjustments[1]\n",
    "right = plot_adjustments[2]\n",
    "top = plot_adjustments[3]\n",
    "\n",
    "plt.subplots_adjust(left,bottom,right, top)\n",
    "\n",
    "ax.legend(fontsize = 14, loc=\"lower right\")\n",
    "\n",
    "plt.savefig(save_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_data = np.load(data_dir + \"ambiguous_data_tau_amplitude_F_signal_noise.npy\")\n",
    "ambig_amplitude_conditions = [\"S\", \"S/M\", \"M\", \"M/L\", \"L\"]\n",
    "ambig_time_constant_conditions = [\"equal_sharp\", \"equal_medium\", \"equal_wide\", \"wide_sharp_negative_skew\", \"wide_medium_negative_skew\",\"medium_sharp_negative_skew\",\"sharp_wide_positive_skew\", \"medium_wide_positive_skew\" ,\"sharp_medium_positive_skew\"]\n",
    "\n",
    "ambiguous_conditions = [\"S/M\", \"M/L\", \"equal_medium\", \"wide_medium_negative_skew\", \"medium_sharp_negative_skew\", \"medium_wide_positive_skew\", \"sharp_medium_positive_skew\"]\n",
    "\n",
    "samples_per_condition = 1000\n",
    "samples_per_ambiguous_condition = 400\n",
    "\n",
    "ambig_cluster_dict = get_index_per_class(ambig_amplitude_conditions,\n",
    "                                         ambig_time_constant_conditions, \n",
    "                                         ambiguous_conditions, \n",
    "                                         samples_per_condition, \n",
    "                                         samples_per_ambiguous_condition)\n",
    "\n",
    "\n",
    "# Clusters in our dataset\n",
    "ambig_clusters_ordered = list(range(0,len(ambig_cluster_dict)+1))\n",
    "\n",
    "\n",
    "# We have 9 clusters for each amplitude and we want them to be plotted in a nice grid format with different\n",
    "# not overlapping in rows thats why we allocate 2 rows and 5 columns for each amplitude \n",
    "ambig_layout_label_mapping = labels_to_layout_mapping(ambig_clusters_ordered, 9, (2,5))\n",
    "\n",
    "\n",
    "\n",
    "functions_for_plotting.plot_clusters(ambig_validation_set, # the dataset \n",
    "                                     ambig_true_labels[ambig_train_fold_indices[1]], # the true labels for the dataset \n",
    "                                     ambig_valid_labels[k_clusters],  # the clustered labels \n",
    "                                     10, # the number of rows in the grid \n",
    "                                     5, # the number of columns in the grid \n",
    "                                     ambig_layout_label_mapping, # our layout mapping \n",
    "                                     figsize=(40,30), # the figsize\n",
    "                                     n_bursts = 100, # the number of bursts you want to plot for each cluster \n",
    "                                     y_lim = (0,16), # the y_lim\n",
    "                                     save_file=save_file_clusters, # the file you want to save the plot \n",
    "                                     subplot_adjustments= [0.05,0.93,0.02,0.92,0.9, 0.2], # adjustments for suplots and overall spacing (tricky) \n",
    "                                     plot_mean=False, # plot the mean of each cluster ? \n",
    "                                     title= \"Validation Set Clusters \\n k=%d, $\\lambda$=%s\" % (k,str(reg))) # title of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "reg = 10\n",
    "k_clusters = 35\n",
    "save_file_clusters = \"PS_vs_F1_clusters_k=%d_reg=%s_clear_balanced_true_kclusters=%d_validation.pdf\" % (k,str(reg),k_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_for_plotting.plot_clusters(clear_validation_set, clear_true_labels[clear_train_fold_indices[1]],clear_valid_labels[k_clusters], 3,4, clear_layout_label_mapping,figsize=(20,20),n_bursts = 100,y_lim = (0,16),save_file=save_file_clusters ,subplot_adjustments= [0.05,0.95,0.03,0.9,0.4, 0.15], plot_mean=False, title= \"Validation Set Clusters \\n k=%d, $\\lambda$=%s\" % (k,str(reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "reg = 0.005\n",
    "k_clusters = 17\n",
    "save_file_clusters = \"F1_clusters_k=%d_reg=%s_ambig_balanced_true_kclusters=%d.pdf\" % (k,str(reg),k_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_for_plotting.plot_clusters(ambig_validation_set, ambig_true_labels[ambig_train_fold_indices[1]],ambig_valid_labels[k_clusters], 10,5, ambig_layout_label_mapping,figsize=(40,30),n_bursts = 100,y_lim = (0,16),save_file=save_file_clusters ,subplot_adjustments= [0.05,0.93,0.02,0.92,0.9, 0.2], plot_mean=False, title= \"Validation Set Clusters \\n k=%d, $\\lambda$=%s\" % (k,str(reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_combinations = list(product([8,9,10,11,12,13,14], repeat=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score_per_k = get_F1_score_per_k(data, train_fold_indices[0], train_fold_indices[1], train_labels, valid_labels,combination_type = \"own\" ,own_combinations = own_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score_per_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = np.argmax(list(F1_score_per_k.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = np.asarray(list(F1_score_per_k.keys()))[argmax]\n",
    "f1_score = np.asarray(list(F1_score_per_k.values()))[argmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score_per_k[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral_clustering import calculate_dist_matrix\n",
    "from spectral_clustering import construct_knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix, sorted_dist_matrix = calculate_dist_matrix(clear_data, \"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = construct_knn_graph(dist_matrix,sorted_dist_matrix,k=10, mutual = False, weighting = \"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600pt\" height=\"600pt\" viewBox=\"0 0 600 600\" version=\"1.1\">\n",
       "<g id=\"surface2\">\n",
       "<rect x=\"0\" y=\"0\" width=\"600\" height=\"600\" style=\"fill:rgb(100%,100%,100%);fill-opacity:1;stroke:none;\"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 193.335938 579.40625 L 20 334.625 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 193.335938 579.40625 L 460.921875 580 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 193.335938 579.40625 L 350.914062 343.433594 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 20 334.625 L 217.8125 271.761719 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 20 334.625 L 193.574219 63.96875 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 217.8125 271.761719 L 580 373.921875 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 217.8125 271.761719 L 333.738281 20 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 580 373.921875 L 460.921875 580 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 580 373.921875 L 522.632812 84.070312 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 460.921875 580 L 465.203125 225.347656 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 350.914062 343.433594 L 333.738281 20 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 350.914062 343.433594 L 522.632812 84.070312 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 193.574219 63.96875 L 522.632812 84.070312 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 193.574219 63.96875 L 465.203125 225.347656 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 333.738281 20 L 465.203125 225.347656 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 203.335938 579.40625 C 203.335938 584.929688 198.859375 589.40625 193.335938 589.40625 C 187.816406 589.40625 183.335938 584.929688 183.335938 579.40625 C 183.335938 573.882812 187.816406 569.40625 193.335938 569.40625 C 198.859375 569.40625 203.335938 573.882812 203.335938 579.40625 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 30 334.625 C 30 340.148438 25.523438 344.625 20 344.625 C 14.476562 344.625 10 340.148438 10 334.625 C 10 329.101562 14.476562 324.625 20 324.625 C 25.523438 324.625 30 329.101562 30 334.625 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 227.8125 271.761719 C 227.8125 277.28125 223.332031 281.761719 217.8125 281.761719 C 212.289062 281.761719 207.8125 277.28125 207.8125 271.761719 C 207.8125 266.238281 212.289062 261.761719 217.8125 261.761719 C 223.332031 261.761719 227.8125 266.238281 227.8125 271.761719 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 590 373.921875 C 590 379.445312 585.523438 383.921875 580 383.921875 C 574.476562 383.921875 570 379.445312 570 373.921875 C 570 368.398438 574.476562 363.921875 580 363.921875 C 585.523438 363.921875 590 368.398438 590 373.921875 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 470.921875 580 C 470.921875 585.523438 466.445312 590 460.921875 590 C 455.398438 590 450.921875 585.523438 450.921875 580 C 450.921875 574.476562 455.398438 570 460.921875 570 C 466.445312 570 470.921875 574.476562 470.921875 580 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 360.914062 343.433594 C 360.914062 348.957031 356.433594 353.433594 350.914062 353.433594 C 345.390625 353.433594 340.914062 348.957031 340.914062 343.433594 C 340.914062 337.910156 345.390625 333.433594 350.914062 333.433594 C 356.433594 333.433594 360.914062 337.910156 360.914062 343.433594 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 203.574219 63.96875 C 203.574219 69.492188 199.097656 73.96875 193.574219 73.96875 C 188.050781 73.96875 183.574219 69.492188 183.574219 63.96875 C 183.574219 58.445312 188.050781 53.96875 193.574219 53.96875 C 199.097656 53.96875 203.574219 58.445312 203.574219 63.96875 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 343.738281 20 C 343.738281 25.523438 339.261719 30 333.738281 30 C 328.214844 30 323.738281 25.523438 323.738281 20 C 323.738281 14.476562 328.214844 10 333.738281 10 C 339.261719 10 343.738281 14.476562 343.738281 20 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 532.632812 84.070312 C 532.632812 89.59375 528.15625 94.070312 522.632812 94.070312 C 517.109375 94.070312 512.632812 89.59375 512.632812 84.070312 C 512.632812 78.546875 517.109375 74.070312 522.632812 74.070312 C 528.15625 74.070312 532.632812 78.546875 532.632812 84.070312 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 475.203125 225.347656 C 475.203125 230.871094 470.726562 235.347656 465.203125 235.347656 C 459.679688 235.347656 455.203125 230.871094 455.203125 225.347656 C 455.203125 219.824219 459.679688 215.347656 465.203125 215.347656 C 470.726562 215.347656 475.203125 219.824219 475.203125 225.347656 \"/>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<igraph.drawing.Plot at 0x1a21474910>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "image/svg+xml": {
       "isolated": true
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import igraph as ig\n",
    "import cairocffi as cairo\n",
    "g = ig.Graph.Famous(\"petersen\")\n",
    "ig.plot(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {}\n",
    "for i in range(10):\n",
    "    color_dict[i] = \"C%d\" % i\n",
    "\n",
    "color_dict[10] = \"black\"\n",
    "color_dict[11] = \"white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ig.Graph.Adjacency((A > 0).tolist())\n",
    "\n",
    "# Add edge weights and node labels.\n",
    "g.es['weight'] = A[A.nonzero()]\n",
    "#g.vs['color'] = [color_dict[i] for i in clear_true_labels]  # or a.index/a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = g.layout_lgl()\n",
    "\n",
    "#ig.plot(g,layout = \"layout_lgl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig.plot(g, layout = layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "import time\n",
    "start = time.time()\n",
    "dist_matrix = np.zeros((len(clear_data[0:1]), len(clear_data)))\n",
    "for i in range(len(clear_data[0:1])):\n",
    "    for j in range(len(clear_data)):\n",
    "        if i == j:\n",
    "            continue  # self-distance is 0.0\n",
    "        if i > j:\n",
    "            dist_matrix[i, j] = dist_matrix[j, i]  # re-use earlier calc\n",
    "        \n",
    "        dist_matrix[i, j] = wasserstein_distance(clear_data[i], clear_data[j])\n",
    "elapsed_time_fl = (time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.703246116638184"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time_fl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
