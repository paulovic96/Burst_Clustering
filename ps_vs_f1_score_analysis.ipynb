{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paule/anaconda3/envs/bon17/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.nearest_centroid module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from spectral_clustering import spectral_clustering\n",
    "import functions_for_plotting\n",
    "from asymmetric_laplacian_distribution import get_index_per_class, get_labels, labels_to_layout_mapping\n",
    "from sklearn.cluster import KMeans\n",
    "import training_set_split\n",
    "import seaborn as sns\n",
    "import prediction_strength\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "from prediction_strength import get_F1_score_per_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and True Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------- DATA ------------------------------------------------------------------\n",
    "data_dir = \"data/\"\n",
    "\n",
    "clear_data = np.load(data_dir + \"clearly_separated_data_F_signal_noise.npy\")\n",
    "ambig_data = np.load(data_dir + \"ambiguous_data_tau_amplitude_F_signal_noise.npy\")\n",
    "#np.load(data_dir + \"ambiguous_data_tau_amplitude_F_signal_noise.npy\") #np.load(data_dir + \"clearly_separated_data_F_signal_noise.npy\")\n",
    "\n",
    "clear_amplitude_conditions = [\"S\", \"M\", \"L\"]  #[\"S\", \"S/M\", \"M\", \"M/L\", \"L\"] #[\"S\", \"M\", \"L\"]\n",
    "ambig_amplitude_conditions = [\"S\", \"S/M\", \"M\", \"M/L\", \"L\"]\n",
    "\n",
    "clear_time_constant_conditions = [\"equal_sharp\", \"equal_wide\", \"wide_sharp_negative_skew\", \"sharp_wide_positive_skew\"]\n",
    "ambig_time_constant_conditions = [\"equal_sharp\", \"equal_medium\", \"equal_wide\", \"wide_sharp_negative_skew\", \"wide_medium_negative_skew\",\"medium_sharp_negative_skew\",\"sharp_wide_positive_skew\", \"medium_wide_positive_skew\" ,\"sharp_medium_positive_skew\"]\n",
    "\n",
    "#[\"equal_sharp\", \"equal_medium\", \"equal_wide\", \"wide_sharp_negative_skew\", \"wide_medium_negative_skew\",\"medium_sharp_negative_skew\",\"sharp_wide_positive_skew\", \"medium_wide_positive_skew\" ,\"sharp_medium_positive_skew\"]\n",
    "#[\"equal_sharp\", \"equal_wide\", \"wide_sharp_negative_skew\", \"sharp_wide_positive_skew\"]\n",
    "\n",
    "ambiguous_conditions = [\"S/M\", \"M/L\", \"equal_medium\", \"wide_medium_negative_skew\", \"medium_sharp_negative_skew\", \"medium_wide_positive_skew\", \"sharp_medium_positive_skew\"]\n",
    "\n",
    "samples_per_condition = 1000\n",
    "samples_per_ambiguous_condition = 400\n",
    "\n",
    "ambig_cluster_dict = get_index_per_class(ambig_amplitude_conditions,ambig_time_constant_conditions, ambiguous_conditions, samples_per_condition, samples_per_ambiguous_condition)\n",
    "clear_cluster_dict = get_index_per_class(clear_amplitude_conditions,clear_time_constant_conditions, [], samples_per_condition, samples_per_ambiguous_condition)\n",
    "\n",
    "\n",
    "clear_true_labels = get_labels(clear_data, clear_cluster_dict)\n",
    "ambig_true_labels = get_labels(ambig_data, ambig_cluster_dict)\n",
    "\n",
    "clear_clusters_ordered = list(range(0,len(clear_cluster_dict)+1))\n",
    "clear_layout_label_mapping = labels_to_layout_mapping(clear_clusters_ordered, 4, (1,4)) #labels_to_layout_mapping(clusters_ordered, 4, (1,4)) #labels_to_layout_mapping(clusters_ordered, 9, (2,5))\n",
    "\n",
    "ambig_clusters_ordered = list(range(0,len(ambig_cluster_dict)+1))\n",
    "ambig_layout_label_mapping = labels_to_layout_mapping(ambig_clusters_ordered, 9, (2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Balanced Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_train_fold_indices, _ = training_set_split.get_training_folds(clear_data, clear_cluster_dict,cluster_split=\"balanced\",folds = 2)\n",
    "ambig_train_fold_indices, _ = training_set_split.get_training_folds(ambig_data, ambig_cluster_dict,cluster_split=\"balanced\",folds = 2)\n",
    "\n",
    "\n",
    "clear_training_set = clear_data[clear_train_fold_indices[0]]\n",
    "clear_validation_set = clear_data[clear_train_fold_indices[1]]\n",
    "\n",
    "ambig_training_set = ambig_data[ambig_train_fold_indices[0]]\n",
    "ambig_validation_set = ambig_data[ambig_train_fold_indices[1]]\n",
    "\n",
    "clear_true_labels_training = clear_true_labels[clear_train_fold_indices[0]]\n",
    "clear_true_labels_validation = clear_true_labels[clear_train_fold_indices[1]]\n",
    "\n",
    "ambig_true_labels_training = ambig_true_labels[ambig_train_fold_indices[0]]\n",
    "ambig_true_labels_validation = ambig_true_labels[ambig_train_fold_indices[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Unbalanced Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(42)\n",
    "clear_clusters = np.arange(12)\n",
    "ambig_clusters = np.arange(45)\n",
    "np.random.shuffle(clear_clusters)\n",
    "np.random.shuffle(ambig_clusters)\n",
    "\n",
    "clear_training_clusters = clear_clusters[0:6]\n",
    "ambig_training_clusters = ambig_clusters[0:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_train_fold_indices, clear_valid_fold_indices = training_set_split.get_training_folds(clear_data, clear_cluster_dict,cluster_split=\"unbalanced\",training_clusters = clear_training_clusters,folds = 2)\n",
    "ambig_train_fold_indices, ambig_valid_fold_indices = training_set_split.get_training_folds(ambig_data, ambig_cluster_dict,cluster_split=\"unbalanced\",training_clusters = ambig_training_clusters, folds = 2)\n",
    "\n",
    "\n",
    "clear_training_set = clear_data[clear_train_fold_indices]\n",
    "clear_validation_set = clear_data[clear_valid_fold_indices]\n",
    "\n",
    "ambig_training_set = ambig_data[ambig_train_fold_indices]\n",
    "ambig_validation_set = ambig_data[ambig_train_fold_indices]\n",
    "\n",
    "clear_true_labels_training = clear_true_labels[clear_train_fold_indices]\n",
    "clear_true_labels_validation = clear_true_labels[clear_valid_fold_indices]\n",
    "\n",
    "ambig_true_labels_training = ambig_true_labels[ambig_train_fold_indices]\n",
    "ambig_true_labels_validation = ambig_true_labels[ambig_valid_fold_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(clear_true_labels_training,return_counts = True))\n",
    "print(np.unique(clear_true_labels_validation,return_counts = True))\n",
    "print(np.unique(ambig_true_labels_training,return_counts = True))\n",
    "print(np.unique(ambig_true_labels_validation,return_counts = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction strength vs. F1 score \n",
    "- True positives: same cluster in  training, same cluster in validation\n",
    "- False positives: different cluster in training, same cluster in validation\n",
    "- True negatives: different cluster in training, different cluster in validation\n",
    "- False negatives: same cluster in training, different cluster in validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "reg = 1\n",
    "\n",
    "clear_prediction_strength_dir = \"Toy_data/Clearly_Separated/Prediction_Strength/\"\n",
    "ambig_prediction_strength_dir = \"Toy_data/Ambiguous/Ambiguous_Tau_Amplitude/Prediction_Strength/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels\n",
    "### balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_training_set_labels = np.load(clear_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "clear_validation_set_labels = np.load(clear_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "ambig_training_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "ambig_validation_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "clear_train_labels = {}\n",
    "clear_valid_labels = {}\n",
    "for i, labels in enumerate(clear_training_set_labels):\n",
    "    clear_train_labels[i+1] = labels\n",
    "    clear_valid_labels[i+1] = clear_validation_set_labels[i]\n",
    "    \n",
    "ambig_train_labels = {}\n",
    "ambig_valid_labels = {}\n",
    "for i, labels in enumerate(ambig_training_set_labels):\n",
    "    ambig_train_labels[i+1] = labels\n",
    "    ambig_valid_labels[i+1] = ambig_validation_set_labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score & Prediction Strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Data\n",
    "- F1 Score\n",
    "- Prediction Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "clear_F1_score_per_k = get_F1_score_per_k(clear_data, clear_train_fold_indices[0], clear_train_fold_indices[1], clear_train_labels, clear_valid_labels, combination_type = \"true\" ,true_train_labels = clear_true_labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_prediction_strengths_per_k,_ = prediction_strength.get_prediction_strength_per_k(clear_data, clear_train_fold_indices[0], clear_train_fold_indices[1], clear_train_labels, clear_valid_labels, per_sample = False, true_train_labels = clear_true_labels_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguous Data\n",
    "- F1 Score\n",
    "- Prediction Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "ambig_F1_score_per_k = get_F1_score_per_k(ambig_data, ambig_train_fold_indices[0], ambig_train_fold_indices[1], ambig_train_labels, ambig_valid_labels,combination_type = \"true\" ,true_train_labels = ambig_true_labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_prediction_strengths_per_k,_ = prediction_strength.get_prediction_strength_per_k(ambig_data, ambig_train_fold_indices[0], ambig_train_fold_indices[1], ambig_train_labels, ambig_valid_labels, per_sample = False, true_train_labels = ambig_true_labels_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Clustres in Ambiguous Data\n",
    "- F1 Score\n",
    "- Prediction Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_from_ambig_dataset, counts = np.unique(ambig_true_labels, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_clusters_from_ambig = clusters_from_ambig_dataset[np.where(counts!= 400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_clusters_from_ambig_idx_validation = np.where(np.isin(ambig_true_labels_validation,clear_clusters_from_ambig) == True)[0]\n",
    "clear_clusters_from_ambig_idx_training = np.where(np.isin(ambig_true_labels_training,clear_clusters_from_ambig) == True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_clear_train_inidices = ambig_train_fold_indices[0][clear_clusters_from_ambig_idx_training]\n",
    "ambig_clear_valid_inidices = ambig_train_fold_indices[1][clear_clusters_from_ambig_idx_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_clear_valid_labels = {}\n",
    "for i, labels in enumerate(ambig_validation_set_labels):\n",
    "    ambig_clear_valid_labels[i+1] = labels[clear_clusters_from_ambig_idx_validation]\n",
    "\n",
    "ambig_clear_true_train_labels = ambig_true_labels_training[clear_clusters_from_ambig_idx_training]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "ambig_F1_score_per_k_clear_clusters = get_F1_score_per_k(ambig_data, ambig_clear_train_inidices, ambig_clear_valid_inidices, None, ambig_clear_valid_labels,combination_type = \"true\" ,true_train_labels = ambig_clear_true_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_prediction_strengths_per_k,_ = prediction_strength.get_prediction_strength_per_k(ambig_data, ambig_clear_train_inidices, ambig_clear_valid_inidices, None, ambig_clear_valid_labels, per_sample = False, true_train_labels = ambig_clear_true_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of  Clusters for different regularizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "regs = [None, 0.01, 0.1, 1, 10] \n",
    "clear_prediction_strength_dir = \"Toy_data/Clearly_Separated/Prediction_Strength/\"\n",
    "ambig_prediction_strength_dir = \"Toy_data/Ambiguous/Ambiguous_Tau_Amplitude/Prediction_Strength/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ambig_data\n",
    "train_fold_indices = ambig_train_fold_indices[0]\n",
    "valid_fold_indices = ambig_train_fold_indices[1]\n",
    "true_labels = ambig_true_labels\n",
    "true_train_labels = ambig_true_labels_training\n",
    "true_valid_labels = ambig_true_labels_validation\n",
    "\n",
    "\n",
    "label_dir = ambig_prediction_strength_dir\n",
    "save_file = \"F1_k=%d_reg=%s_ambig_balanced_true_SSIM_EUCLIDEAN\" \n",
    "\n",
    "calculate_F1_for_clear_clusters_in_ambig_data = True\n",
    "save_file_clear_clusters = \"F1_clear_clusters_k=%d_reg=%s_ambig_balanced_true_SSIM_EUCLIDEAN\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n",
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "for reg in regs:\n",
    "    \n",
    "    training_set_labels_i = np.load(label_dir + \"Labels/labels_k=%d_reg=%s_SSIM_EUCLIDEAN_training.npy\" % (k, str(reg)))\n",
    "    validation_set_labels_i = np.load(label_dir + \"Labels/labels_k=%d_reg=%s_SSIM_EUCLIDEAN_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "    train_labels_dict = {}\n",
    "    valid_labels_dict = {}\n",
    "    \n",
    "    for i, labels in enumerate(training_set_labels_i):\n",
    "        train_labels_dict[i+1] = labels\n",
    "        valid_labels_dict[i+1] = validation_set_labels_i[i]\n",
    "        \n",
    "    F1_score_per_k_i = get_F1_score_per_k(data, train_fold_indices, valid_fold_indices, train_labels_dict, valid_labels_dict, combination_type = \"true\" ,true_train_labels = true_train_labels)    \n",
    "    np.save(save_file % (k,str(reg)), F1_score_per_k_i) \n",
    "    \n",
    "    if calculate_F1_for_clear_clusters_in_ambig_data:\n",
    "        clusters_from_ambig_dataset, counts = np.unique(true_labels, return_counts = True)\n",
    "        clear_clusters_from_ambig = clusters_from_ambig_dataset[np.where(counts == 1000)]\n",
    "        clear_clusters_from_ambig_idx_validation = np.where(np.isin(true_valid_labels,clear_clusters_from_ambig) == True)[0]\n",
    "        clear_clusters_from_ambig_idx_training = np.where(np.isin(true_train_labels,clear_clusters_from_ambig) == True)[0]\n",
    "\n",
    "        clear_train_inidices = train_fold_indices[clear_clusters_from_ambig_idx_training]\n",
    "        clear_valid_inidices = valid_fold_indices[clear_clusters_from_ambig_idx_validation]\n",
    "\n",
    "        clear_valid_labels_dict = {}\n",
    "        for i, labels in enumerate(validation_set_labels_i):\n",
    "            clear_valid_labels_dict[i+1] = labels[clear_clusters_from_ambig_idx_validation]\n",
    "\n",
    "        clear_true_train_labels = true_train_labels[clear_clusters_from_ambig_idx_training]    \n",
    "        \n",
    "        F1_score_per_k_i_clear_clusters = get_F1_score_per_k(data, clear_train_inidices, clear_valid_inidices, None, clear_valid_labels_dict, combination_type = \"true\" ,true_train_labels = clear_true_train_labels)\n",
    "        \n",
    "        np.save(save_file_clear_clusters % (k,str(reg)),F1_score_per_k_i_clear_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Mean Prediction Strength vs. F1 Score / F1 Score vs. F1 Score for clear clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20,10)\n",
    "plot_adjustments = [0.05,0.08,0.95, 0.91]\n",
    "k=10\n",
    "configuration = \"k=%d - reg=%s\" % (k,str(reg))\n",
    "#save_file = \"PS_vs_F1_k=%d_reg=%s_ambig_balanced_true.pdf\" % (k,str(reg))\n",
    "save_file = \"F1_clear_clusters_k=%d_reg=%s_ambig_balanced_true.pdf\" % (k,str(reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_strengths_per_k = ambig_prediction_strengths_per_k\n",
    "F1_score_per_k = ambig_F1_score_per_k\n",
    "F1_score_per_k_clear_clusters = ambig_F1_score_per_k_clear_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "k_clusters = list(F1_score_per_k.keys())\n",
    "\n",
    "mean_prediction_strengths = []\n",
    "err_prediction_strengths = []\n",
    "min_prediction_strengths = []\n",
    "F1_scores = []\n",
    "F1_scores_clear_clusters = []\n",
    "\n",
    "for k in k_clusters:\n",
    "    #mean_prediction_strengths.append(np.mean(prediction_strengths_per_k[k]))\n",
    "    #err_prediction_strengths.append(np.std(prediction_strengths_per_k[k]))\n",
    "    #min_prediction_strengths.append(np.amin(prediction_strengths_per_k[k]))\n",
    "    F1_scores.append(F1_score_per_k[k])\n",
    "    F1_scores_clear_clusters.append(F1_score_per_k_clear_clusters[k])\n",
    "\n",
    "\n",
    "#upper_err = np.asarray(err_prediction_strengths) - np.maximum(0,(np.asarray(err_prediction_strengths)+np.asarray(mean_prediction_strengths)-1))\n",
    "#lower_err = np.asarray(err_prediction_strengths)\n",
    "#err = np.stack((lower_err,upper_err), axis=0)\n",
    "\n",
    "ax.plot(k_clusters, F1_scores, \"o-\", label=\"F1-Scores\",color = \"C0\",linewidth=3)\n",
    "ax.plot(k_clusters, F1_scores_clear_clusters, \"o-\", label=\"F1-Scores (Clear Clusters)\",color = \"C03\",linewidth=3)\n",
    "#ax.plot(k_clusters, mean_prediction_strengths, \"o-\",label=\"Mean PS\",color = \"C0\",linewidth=3)\n",
    "#ax.plot(k_clusters, min_prediction_strengths, \"o-\", color = \"C01\", label=\"Min PS\",linewidth=3)\n",
    "\n",
    "argmax_f1 = np.argmax(F1_scores[1:]) + 1\n",
    "argmax_f1_clear_clusters = np.argmax(F1_scores_clear_clusters[1:]) + 1\n",
    "#argmax_mean_ps = np.argmax(mean_prediction_strengths[1:]) +1\n",
    "#argmax_min_ps = np.argmax(min_prediction_strengths[1:])+1\n",
    "\n",
    "\n",
    "#if np.abs(argmax_mean_ps-argmax_f1) <8:\n",
    "if np.abs(argmax_f1_clear_clusters-argmax_f1) <8:\n",
    "    #if F1_scores[argmax_f1] <= mean_prediction_strengths[argmax_mean_ps]:\n",
    "    if F1_scores[argmax_f1] <= F1_scores_clear_clusters[argmax_f1_clear_clusters]:\n",
    "        ps_shift = 0.03\n",
    "        f1_shift = 0\n",
    "    else:\n",
    "        ps_shift = 0\n",
    "        f1_shift = 0.03\n",
    "else:\n",
    "    ps_shift = 0\n",
    "    f1_shift = 0\n",
    "\n",
    "#ax.annotate(\"#%d|Score=%.3f\" % (argmax_mean_ps+1, mean_prediction_strengths[argmax_mean_ps]), (k_clusters[argmax_mean_ps] - 1, mean_prediction_strengths[argmax_mean_ps] + 0.03 + ps_shift),fontsize = 16, color = \"C0\")\n",
    "ax.annotate(\"#%d|Score=%.3f\" % (argmax_f1+1, F1_scores[argmax_f1]), (k_clusters[argmax_f1] - 1, F1_scores[argmax_f1] + 0.03 + f1_shift), fontsize=16, color = \"C0\")\n",
    "ax.annotate(\"#%d|Score=%.3f\" % (argmax_f1_clear_clusters+1, F1_scores_clear_clusters[argmax_f1_clear_clusters]), (k_clusters[argmax_f1_clear_clusters] - 1, F1_scores_clear_clusters[argmax_f1_clear_clusters] + 0.03 + ps_shift),fontsize = 16, color = \"C03\")\n",
    "#ax.annotate(\"#%d|Score=%.3f\" % (argmax_min_ps+1, min_prediction_strengths[argmax_min_ps]), (k_clusters[argmax_min_ps] - 1, min_prediction_strengths[argmax_min_ps] + 0.03), fontsize=16, color = \"C01\")\n",
    "\n",
    "#title = \"Prediction Strength vs. F1-Score for Clustering with k Clusters \\n\" + configuration \n",
    "title = \"F1-Score for Clustering with k Clusters \\n\" + configuration \n",
    "\n",
    "ax.set_title(title, fontsize=22, pad=20)\n",
    "ax.set_xticks(k_clusters)\n",
    "ax.set_xlabel(\"# Number of clusters\", fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"Score\", fontsize=18, labelpad=10),\n",
    "ax.set_ylim((0, 1.1))\n",
    "ax.tick_params(axis='y',labelsize=14)\n",
    "ax.tick_params(axis='x',labelsize=14)\n",
    "\n",
    "ax.set_yticks(np.arange(0, 1.1,0.1))\n",
    "left = plot_adjustments[0]\n",
    "bottom = plot_adjustments[1]\n",
    "right = plot_adjustments[2]\n",
    "top = plot_adjustments[3]\n",
    "\n",
    "plt.subplots_adjust(left,bottom,right, top)\n",
    "\n",
    "ax.legend(fontsize = 14, loc=\"lower right\")\n",
    "\n",
    "plt.savefig(save_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot F1 score for different regularizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20,10)\n",
    "plot_adjustments = [0.05,0.08,0.95, 0.91]\n",
    "save_file = \"Ambig_F1_clear_clusters_regularization_comparison_None_to_100_k=10_SSIM_EUCLIDEAN.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "regs = regs = [None, 0.01, 0.1, 1, 10] \n",
    "configuration = \"SC-Configuration: k=%d\" % (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "product('ABCD', repeat=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_scores_clear_clusters_reg = {}\n",
    "for reg in regs: \n",
    "    f1_dict = np.load(\"F1_clear_clusters_k=%d_reg=%s_ambig_balanced_true_SSIM_EUCLIDEAN.npy\" % (k,str(reg)),allow_pickle=True).item()\n",
    "    k_clusters = list(f1_dict.keys())\n",
    "    F1_scores = []\n",
    "    for i in k_clusters:\n",
    "        #mean_prediction_strengths.append(np.mean(prediction_strengths_per_k[k]))\n",
    "        #err_prediction_strengths.append(np.std(prediction_strengths_per_k[k]))\n",
    "        #min_prediction_strengths.append(np.amin(prediction_strengths_per_k[k]))\n",
    "        F1_scores.append(f1_dict[i])\n",
    "    F1_scores_clear_clusters_reg[reg] = F1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "k_clusters = range(1,50)\n",
    "for i,reg in enumerate(regs):\n",
    "    F1_scores = F1_scores_clear_clusters_reg[reg]\n",
    "    ax.plot(k_clusters, F1_scores, \"o-\", label=\"reg=%s\" % str(reg),color = \"C0%d\" % i,linewidth=3)\n",
    "\n",
    "    argmax_f1 = np.argmax(F1_scores[1:]) + 1\n",
    "\n",
    "    ax.annotate(\"#%d|Score=%.3f\" % (argmax_f1+1, F1_scores[argmax_f1]), (k_clusters[argmax_f1] - 1, F1_scores[argmax_f1] + 0.03 - i*0.03), fontsize=16, color = \"C0%d\" % i)\n",
    "\n",
    "\n",
    "title = \"F1-Score of Clear Clusters for Clustering with k Clusters \\n\" + configuration \n",
    "\n",
    "ax.set_title(title, fontsize=22, pad=20)\n",
    "ax.set_xticks(k_clusters)\n",
    "ax.set_xlabel(\"# Number of clusters\", fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"F1-Score\", fontsize=18, labelpad=10),\n",
    "ax.set_ylim((0, 1.1))\n",
    "ax.tick_params(axis='y',labelsize=14)\n",
    "ax.tick_params(axis='x',labelsize=14)\n",
    "\n",
    "ax.set_yticks(np.arange(0, 1.1,0.1))\n",
    "left = plot_adjustments[0]\n",
    "bottom = plot_adjustments[1]\n",
    "right = plot_adjustments[2]\n",
    "top = plot_adjustments[3]\n",
    "\n",
    "plt.subplots_adjust(left,bottom,right, top)\n",
    "\n",
    "ax.legend(fontsize = 14, loc=\"lower right\")\n",
    "\n",
    "plt.savefig(save_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ambig_valid_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c21ea9db362d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m functions_for_plotting.plot_clusters(ambig_validation_set, # the dataset \n\u001b[1;32m     28\u001b[0m                                      \u001b[0mambig_true_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mambig_train_fold_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# the true labels for the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                      \u001b[0mambig_valid_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_clusters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# the clustered labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                                      \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# the number of rows in the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                      \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# the number of columns in the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ambig_valid_labels' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/\"\n",
    "ambig_data = np.load(data_dir + \"ambiguous_data_tau_amplitude_F_signal_noise.npy\")\n",
    "ambig_amplitude_conditions = [\"S\", \"S/M\", \"M\", \"M/L\", \"L\"]\n",
    "ambig_time_constant_conditions = [\"equal_sharp\", \"equal_medium\", \"equal_wide\", \"wide_sharp_negative_skew\", \"wide_medium_negative_skew\",\"medium_sharp_negative_skew\",\"sharp_wide_positive_skew\", \"medium_wide_positive_skew\" ,\"sharp_medium_positive_skew\"]\n",
    "\n",
    "ambiguous_conditions = [\"S/M\", \"M/L\", \"equal_medium\", \"wide_medium_negative_skew\", \"medium_sharp_negative_skew\", \"medium_wide_positive_skew\", \"sharp_medium_positive_skew\"]\n",
    "\n",
    "samples_per_condition = 1000\n",
    "samples_per_ambiguous_condition = 400\n",
    "\n",
    "ambig_cluster_dict = get_index_per_class(ambig_amplitude_conditions,\n",
    "                                         ambig_time_constant_conditions, \n",
    "                                         ambiguous_conditions, \n",
    "                                         samples_per_condition, \n",
    "                                         samples_per_ambiguous_condition)\n",
    "\n",
    "ambig_true_labels = get_labels(ambig_data, ambig_cluster_dict)\n",
    "\n",
    "# Clusters in our dataset\n",
    "ambig_clusters_ordered = list(range(0,len(ambig_cluster_dict)+1))\n",
    "\n",
    "\n",
    "# We have 9 clusters for each amplitude and we want them to be plotted in a nice grid format with different\n",
    "# not overlapping in rows thats why we allocate 2 rows and 5 columns for each amplitude \n",
    "ambig_layout_label_mapping = labels_to_layout_mapping(ambig_clusters_ordered, 9, (2,5))\n",
    "\n",
    "ambig_train_fold_indices, _ = training_set_split.get_training_folds(ambig_data, ambig_cluster_dict,cluster_split=\"balanced\",folds = 2)\n",
    "\n",
    "ambig_training_set = ambig_data[ambig_train_fold_indices[0]]\n",
    "ambig_validation_set = ambig_data[ambig_train_fold_indices[1]]\n",
    "\n",
    "\n",
    "ambig_true_labels_training = ambig_true_labels[ambig_train_fold_indices[0]]\n",
    "ambig_true_labels_validation = ambig_true_labels[ambig_train_fold_indices[1]]    \n",
    "\n",
    "\n",
    "k = 10 \n",
    "reg = 1 \n",
    "ambig_prediction_strength_dir = \"Toy_data/Ambiguous/Ambiguous_Tau_Amplitude/Prediction_Strength/\"\n",
    "\n",
    "ambig_training_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "ambig_validation_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "    \n",
    "ambig_train_labels = {}\n",
    "ambig_valid_labels = {}\n",
    "for i, labels in enumerate(ambig_training_set_labels):\n",
    "    ambig_train_labels[i+1] = labels\n",
    "    ambig_valid_labels[i+1] = ambig_validation_set_labels[i]\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "For Clear Dataset:\n",
    "rows = 3\n",
    "columns = 4\n",
    "figsize = (20,20)\n",
    "clear_layout_label_mapping = 4, (1,4)\n",
    "subplot_adjustments = [0.05,0.95,0.03,0.9,0.4, 0.15]\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "k_clusters = 20\n",
    "\n",
    "save_file_clusters = \"F1_clusters_k=%d_reg=%s_ambig_balanced_true_kclusters=%d.pdf\" % (k,str(reg),k_clusters)\n",
    "\n",
    "functions_for_plotting.plot_clusters(ambig_validation_set, # the dataset \n",
    "                                     ambig_true_labels[ambig_train_fold_indices[1]], # the true labels for the dataset \n",
    "                                     ambig_valid_labels[k_clusters],  # the clustered labels \n",
    "                                     10, # the number of rows in the grid \n",
    "                                     5, # the number of columns in the grid \n",
    "                                     ambig_layout_label_mapping, # our layout mapping \n",
    "                                     figsize=(40,30), # the figsize\n",
    "                                     n_bursts = 100, # the number of bursts you want to plot for each cluster \n",
    "                                     y_lim = (0,16), # the y_lim\n",
    "                                     save_file=save_file_clusters, # the file you want to save the plot \n",
    "                                     subplot_adjustments= [0.05,0.93,0.02,0.92,0.9, 0.2], # adjustments for suplots and overall spacing (tricky) \n",
    "                                     plot_mean=False, # plot the mean of each cluster ? \n",
    "                                     title= \"Validation Set Clusters \\n k=%d, $\\lambda$=%s\" % (k,str(reg))) # title of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Clear Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_from_ambig_dataset, counts = np.unique(ambig_true_labels, return_counts = True)\n",
    "clear_clusters_from_ambig = clusters_from_ambig_dataset[np.where(counts!= 400)]\n",
    "clear_clusters_from_ambig_idx_validation = np.where(np.isin(ambig_true_labels_validation,clear_clusters_from_ambig) == True)[0]\n",
    "clear_clusters_from_ambig_idx_training = np.where(np.isin(ambig_true_labels_training,clear_clusters_from_ambig) == True)[0]\n",
    "ambig_clear_train_inidices = ambig_train_fold_indices[0][clear_clusters_from_ambig_idx_training]\n",
    "ambig_clear_valid_inidices = ambig_train_fold_indices[1][clear_clusters_from_ambig_idx_validation]\n",
    "ambig_clear_valid_labels = {}\n",
    "for i, labels in enumerate(ambig_validation_set_labels):\n",
    "    ambig_clear_valid_labels[i+1] = labels[clear_clusters_from_ambig_idx_validation]\n",
    "\n",
    "ambig_clear_true_train_labels = ambig_true_labels_training[clear_clusters_from_ambig_idx_training]    \n",
    "\n",
    "\n",
    "k = 10 \n",
    "reg = 0.2\n",
    "k_clusters = 17\n",
    "save_file_clusters = \"F1_clear_clusters_k=%d_reg=%s_ambig_balanced_true_kclusters=%d.pdf\" % (k,str(reg),k_clusters)\n",
    "\n",
    "\n",
    "\n",
    "functions_for_plotting.plot_clusters(ambig_validation_set[clear_clusters_from_ambig_idx_validation], # the dataset \n",
    "                                     ambig_true_labels[ambig_clear_valid_inidices], # the true labels for the dataset \n",
    "                                     ambig_clear_valid_labels[k_clusters],  # the clustered labels \n",
    "                                     10, # the number of rows in the grid \n",
    "                                     5, # the number of columns in the grid \n",
    "                                     ambig_layout_label_mapping, # our layout mapping \n",
    "                                     figsize=(40,30), # the figsize\n",
    "                                     n_bursts = 100, # the number of bursts you want to plot for each cluster \n",
    "                                     y_lim = (0,16), # the y_lim\n",
    "                                     save_file=save_file_clusters, # the file you want to save the plot \n",
    "                                     subplot_adjustments= [0.05,0.93,0.02,0.92,0.9, 0.2], # adjustments for suplots and overall spacing (tricky) \n",
    "                                     plot_mean=False, # plot the mean of each cluster ? \n",
    "                                     title= \"Validation Set: Clear Clusters \\n k=%d, $\\lambda$=%s\" % (k,str(reg))) # title of the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
