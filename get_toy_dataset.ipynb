{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from spectral_clustering import spectral_clustering\n",
    "import seaborn as sns\n",
    "import functions_for_plotting\n",
    "from asymmetric_laplacian_distribution import get_index_per_class, get_labels, labels_to_layout_mapping\n",
    "from sklearn.cluster import KMeans\n",
    "import training_set_split\n",
    "import seaborn as sns\n",
    "import prediction_strength\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "from prediction_strength import get_F1_score_per_k\n",
    "from matplotlib.legend import Legend\n",
    "from training_set_split import get_training_folds\n",
    "import wagenaar_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Dataset\n",
    "\n",
    "For validating the method of Spectral Clustering the idea was to generate toy datasets and check whether the method is able to capture known true clusters. In order to have a burst like toy-dataset we decided to use asymmetric laplace distributions for which we adjusted amplitude and time constants. \n",
    "\n",
    "$\\begin{align}f(x,\\mu,\\lambda, \\tau_1, \\tau_2):\\\\   \n",
    "\\text{if } x < \\mu:\\hspace{1mm} &f = \\lambda \\cdot exp(\\tau_1(x-\\mu)) \\\\  \n",
    "\\text{else }: \\hspace{1mm} &f = \\lambda \\cdot exp(-\\tau_2(x-\\mu)) \\\\ \n",
    "\\text{return }:\\hspace{1mm} & f\\end{align}$\n",
    "\n",
    "We came up with three different toy-datasets each with different charactersitics. \n",
    "The values used are inspired by the daily spontaneous activation of cultured neurons after 20 days in vitro taken from Wagenaar, Pine, and Potter's dataset (Wagenaar, Pine & Potter 2006). \n",
    "\n",
    "## Amplitudes: \n",
    "- Small (S) : [1, 2]\n",
    "- Small/Mediun (S_M): [3,4]\n",
    "- Medium (M): [5,7]\n",
    "- Medium/Large (M_L): [8,11]\n",
    "- Large (L): [12,14]  \n",
    "\n",
    "## Time constants: \n",
    "- $\\tau_1 \\ll \\tau_2$ (slow rise - fast fall)\n",
    "- $\\tau_1 < \\tau_2$ (slow rise - medium fall / medium rise - fast fall)\n",
    "- $\\tau_1 \\gg \\tau_2$ (fast rise - slow fall) \n",
    "- $\\tau_1 > \\tau_2$(medium rise - slow fall / fast rise - medium fall)\n",
    "- $\\tau_1  ≈  \\tau_2$ (sharp)\n",
    "- $\\tau_1  ≈  \\tau_2$ (wide)\n",
    "- $\\tau_1  ≈  \\tau_2$ (medium)\n",
    "- $small_\\tau = [0.001, 0.003]$\n",
    "- $medium_\\tau = [0.004, 0.019]$\n",
    "- $large_\\tau = [0.02, 0.5]$\n",
    "\n",
    "\n",
    "The first and most simple one we created was called 'clearly separable'. It was build from a 3x4 combination of amplitude and time constants and contained 12 clusters in total. We used a time range of [0, 3500] and aligned peaks at $\\mu = 1750$\n",
    "\n",
    "### Clearly Separable Dataset: \n",
    "- Amplitudes: S,M,L\n",
    "- Time Constants: $small_\\tau$,$large_\\tau$ \n",
    "    1. $\\tau_1  ≈  \\tau_2$ (sharp)\n",
    "    2. $\\tau_1  ≈  \\tau_2$ (wide)\n",
    "    3. $\\tau_1 \\ll \\tau_2$ (slow rise - fast fall)\n",
    "    4. $\\tau_1 \\gg \\tau_2$ (fast rise - slow fall)\n",
    "\n",
    "\n",
    "For each combination we calculated 1000 samples. For each sample we uniformly,randomly drew the parameters from the coressponding range. Afterwards we added gaussian noise with a std of 0.2. Negative value after adding the noise were set to zero.\n",
    "\n",
    "The second dataset call 'ambiguous' was created in order to introduce some small ambiguity between clusters and make the synthetic data closer to the reality. We therefore introduced the $medium_\\tau$ time constant and the amplitudes Small/Medium and Medium/Large.    \n",
    "With these additional parameters and resulting additional conditions we tried to create datapoints right between the previous clusters. We calculated 400 additional datapoints for each of these conditions.\n",
    "The new ambiguous toy dataset contained 25200 examples from 45 different conditions (for each amplitude 9 time constant combinations).\n",
    "\n",
    "### Ambiguous Dataset:\n",
    "- Amplitudes: S,S_M,M,S_L,L\n",
    "- Time Constants: $small_\\tau$,$large_\\tau$ \n",
    "    1. $\\tau_1  ≈  \\tau_2$ (sharp)\n",
    "    2. $\\tau_1  ≈  \\tau_2$ (medium)\n",
    "    3. $\\tau_1  ≈  \\tau_2$ (wide)\n",
    "    4. $\\tau_1 \\ll \\tau_2$ (slow rise - fast fall)\n",
    "    5. $\\tau_1 < \\tau_2$ (slow rise - medium fall)\n",
    "    6. $\\tau_1 < \\tau_2$ (medium rise - fast fall)\n",
    "    7. $\\tau_1 \\gg \\tau_2$ (fast rise - slow fall) \n",
    "    8. $\\tau_1 > \\tau_2$(medium rise - slow fall) fast rise - medium fall)\n",
    "    9. $\\tau_1 > \\tau_2$(fast rise - medium fall)\n",
    "\n",
    "\n",
    "\n",
    "With respect to the inbalance in amplitudes observed in Wagenaar et al.'s (Wagenaar, Pine & Potter 2006) real \n",
    "world data we created a third dataset. \n",
    "We adapt the distribution of clusters in the clearly separable toy dataset by looking at amplitudes of bursts occuring in Wagenaar et al.'s data at day 20. We considered every burst with an amplitude <= 1 as \"small\" or \"tiny\". This resulted in 13383 'tiny' and 523 'non-tiny' bursts. \n",
    "We uniformly randomly distributed the 13383 'tiny' burst over the 4 small clusters (small amplitude in all time constant conditions). Same was done for the remaining 523 bursts and the remaining 8 other classes.\n",
    "This resulted in a toy dataset with 13906 bursts. For the time range we used  [0, 3411].\n",
    "\n",
    "### Tiny burst informed toy-dataset:\n",
    "- Amplitudes: S,M,L\n",
    "- Time Constants: $small_\\tau$,$large_\\tau$ --> Samples: [S,M,L]\n",
    "    1. $\\tau_1  ≈  \\tau_2$ (sharp) -->  [3298, 56, 67]\n",
    "    2. $\\tau_1  ≈  \\tau_2$ (wide) --> [3401, 54, 73]\n",
    "    3. $\\tau_1 \\ll \\tau_2$ (slow rise - fast fall) --> [3347, 80, 60]\n",
    "    4. $\\tau_1 \\gg \\tau_2$ (fast rise - slow fall) --> [3337, 68, 65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Burst Overloaded Wagenaar Data inspired "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/raw_data/daily_spontanous_dense/day20/\"\n",
    "day20_data = np.load(data_dir + \"data_burst_by_time_day_20.npy\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_RANGE = [0, day20_data.shape[1]]\n",
    "EQUAL_NOISE = [0, 0.2]\n",
    "NOISE_TYPE = \"gaussian\"\n",
    "USE_EQUAL_NOISE = True\n",
    "\n",
    "AMPLITUDE_CONDITIONS = [\"S\", \"M\", \"L\"]\n",
    "TIME_CONSTANT_CONDITIONS = [\"equal_sharp\", \"equal_wide\", \"wide_sharp_negative_skew\", \"sharp_wide_positive_skew\"]\n",
    "AMBIGUOUS_CONDITIONS = []\n",
    "SAMPLES_PER_AMBIGUOUS_CONDITION = 0\n",
    "MU = day20_data.shape[1]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tiny_bursts_in_real_data = len(np.where(np.amax(day20_data,axis=1) <= 1)[0])\n",
    "n_non_tiny_bursts_in_real_data = len(day20_data) - n_tiny_bursts_in_real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tiny Bursts: \", n_tiny_bursts_in_real_data)\n",
    "print(\"Non-Tiny Bursts: \", n_non_tiny_bursts_in_real_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniformly sample the tiny burst conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tiny_burst_conditions = np.random.multinomial(n_tiny_bursts_in_real_data, [1/4.]*4, size=1)[0]\n",
    "non_tiny_burst_conditions =  np.random.multinomial(n_non_tiny_bursts_in_real_data, [1/8.]*8, size=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tiny burst #/condition:\", tiny_burst_conditions)\n",
    "print(\"Non-Tiny burst #/condition:\", non_tiny_burst_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_PER_CONDITION = list(tiny_burst_conditions) + list(non_tiny_burst_conditions)\n",
    "print(SAMPLES_PER_CONDITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_signal, F_signal_noise, noises, param_data = generate_ALF_data(X, AMPLITUDE_CONDITIONS, \n",
    "                                                                 TIME_CONSTANT_CONDITIONS,\n",
    "                                                                 AMBIGUOUS_CONDITIONS, \n",
    "                                                                 SAMPLES_PER_CONDITION,\n",
    "                                                                 SAMPLES_PER_AMBIGUOUS_CONDITION,MU, \n",
    "                                                                 noise_type = NOISE_TYPE, \n",
    "                                                                 equal_noise = USE_EQUAL_NOISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_data.to_csv( \"clear_data_equal_noise=[0,0.2]_tiny_burst_overload_non_tiny_upsampled\" + \"_parameter\" + \".csv\",index=False)\n",
    "#np.save(\"clear_data_equal_noise=[0,0.2]_tiny_burst_overload_non_tiny_upsampled\" + \"_F_signal\",F_signal)\n",
    "#np.save(\"clear_data_equal_noise=[0,0.2]_tiny_burst_overload_non_tiny_upsampled\" + \"_F_signal_noise\", F_signal_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"clear_data_equal_noise=[0,0.2]_tiny_burst_overload_F_signal_noise.npy\")\n",
    "class_dict = get_index_per_class(AMPLITUDE_CONDITIONS, TIME_CONSTANT_CONDITIONS, \n",
    "                                 AMBIGUOUS_CONDITIONS, SAMPLES_PER_CONDITION, \n",
    "                                 SAMPLES_PER_AMBIGUOUS_CONDITION)\n",
    "true_labels = get_labels(data, class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_clusters_ordered = list(range(0,len(class_dict)+1))\n",
    "layout_mapping = labels_to_layout_mapping(clear_clusters_ordered, 4, (1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "columns = 4\n",
    "figsize = (20,20)\n",
    "subplot_adjustments = [0.05,0.95,0.03,0.9,0.4, 0.15]\n",
    "title = \"\"\n",
    "save_file_clusters=\"test.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_for_plotting.plot_clusters(data, # the dataset \n",
    "                                     true_labels, # the true labels for the dataset \n",
    "                                     true_labels,  # the clustered labels \n",
    "                                     rows, # the number of rows in the grid \n",
    "                                     columns, # the number of columns in the grid \n",
    "                                     layout_mapping, # our layout mapping \n",
    "                                     figsize=figsize, # the figsize\n",
    "                                     n_bursts = 100, # the number of bursts you want to plot for each cluster \n",
    "                                     y_lim = (0,16), # the y_lim\n",
    "                                     save_file=save_file_clusters, # the file you want to save the plot \n",
    "                                     subplot_adjustments= subplot_adjustments, # adjustments for suplots and overall spacing (tricky) \n",
    "                                     plot_mean=True, # plot the mean of each cluster ? \n",
    "                                     title= title) # title of the plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
