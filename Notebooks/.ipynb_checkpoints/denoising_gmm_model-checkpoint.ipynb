{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoise Data based on prediction Strength per sample \n",
    "Calculate prediction strength per sample and throw out samples with low prediction strength.\n",
    "Reduce the number of clusters if all samples of one cluster are thrown out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] ='40'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import struct\n",
    "import json\n",
    "import sys\n",
    "import seaborn as sns \n",
    "from scipy import signal, stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.defs.prediction_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over channels:  (13092, 3410)\n",
      "(13092, 3410)\n",
      "(13092, 3410)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../Data/'\n",
    "data_burst_by_time = np.load(data_dir + 'data_burst_by_time.npy').T\n",
    "data_burst_by_time_shuffled = (np.random.permutation(data_burst_by_time))\n",
    "print(\"Averaged over channels: \", data_burst_by_time.shape)\n",
    "print(data_burst_by_time.shape)\n",
    "print(data_burst_by_time_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_burst_by_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = np.load(data_dir + \"50_50_split/train_folds_50_50.npy\") #np.load(data_dir + \"culture_balanced/culture_balanced_training_split.npy\")\n",
    "valid_folds = np.load(data_dir + \"50_50_split/valid_folds_50_50.npy\") #np.load(data_dir + \"culture_balanced/culture_balanced_validation_split.npy\")\n",
    "\n",
    "if len(train_folds.shape)>1:\n",
    "    training_sets = []\n",
    "    validation_sets = []\n",
    "    for i, split in enumerate(train_folds):\n",
    "        training_sets.append(data[split])\n",
    "        validation_sets.append(data[valid_folds[i]])\n",
    "else:\n",
    "    train_folds = [train_folds]\n",
    "    valid_folds = [valid_folds]\n",
    "    training_sets = [data[train_folds]]  #data_burst_by_time[training_split] # extract training bursts from dataset with indices\n",
    "    validation_sets = [data[valid_folds]]  #data_burst_by_time[test_split] # extract validation bursts from dataset with indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 :\n",
      "6546 Bursts in Training Set equal to 50.00 % of the total data. \n",
      "6546 Bursts in Validation Set equal to 50.00 % of the total data. \n",
      "Split 2 :\n",
      "6546 Bursts in Training Set equal to 50.00 % of the total data. \n",
      "6546 Bursts in Validation Set equal to 50.00 % of the total data. \n"
     ]
    }
   ],
   "source": [
    "for i, train_set in enumerate(training_sets):\n",
    "    print(\"Split %d :\" % (i+1))\n",
    "    print(\"%d Bursts in Training Set equal to %.2f %% of the total data. \" % (len(train_set), np.round((len(train_set)/len(data)), 4) * 100))\n",
    "    print(\"%d Bursts in Validation Set equal to %.2f %% of the total data. \" % (len(validation_sets[i]), np.round((len(validation_sets[i])/len(data)), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bics = list(np.load(data_dir + '50_50_split/bics_1.npy',allow_pickle=True)) # contains bic score for training and validation set since 50/50 split one file contains full info about bic \n",
    "scores_1 = list(np.load(data_dir + '50_50_split/scores_1.npy',allow_pickle=True)) # scores for training and validation set \n",
    "scores_2 = list(np.load(data_dir + '50_50_split/scores_1.npy',allow_pickle=True))\n",
    "\n",
    "predictions_valid_1 = list(np.load(data_dir + '50_50_split/predictions_valid_1.npy',allow_pickle=True)) # predictions of validation set \n",
    "predictions_valid_2 = list(np.load(data_dir + '50_50_split/predictions_valid_2.npy',allow_pickle=True))\n",
    "\n",
    "predictions_train_1 = list(np.load(data_dir + '50_50_split/predictions_train_1.npy',allow_pickle=True)) # predictions of training set \n",
    "predictions_train_2 = list(np.load(data_dir + '50_50_split/predictions_train_2.npy',allow_pickle=True)) \n",
    "\n",
    "predictions_valid_fitted_1 = list(np.load(data_dir + '50_50_split/predictions_valid_fitted_1.npy',allow_pickle=True)) # predictions of validation set after fitting gmm on it\n",
    "predictions_valid_fitted_2 = list(np.load(data_dir + '50_50_split/predictions_valid_fitted_2.npy',allow_pickle=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "n_folds = 2\n",
    "\n",
    "\n",
    "train_fold_labels_gmm = np.stack((predictions_train_1,predictions_train_2),axis = 0)\n",
    "valid_fold_labels_gmm = np.stack((predictions_valid_fitted_1,predictions_valid_fitted_2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_strengths_cv_gmm_per_sample = list(np.load(data_dir + '50_50_split/prediction_strength_gmm_per_sample.npy',allow_pickle=True))\n",
    "predictions_strengths_cv_gmm = list(np.load(data_dir + '50_50_split/prediction_strength_gmm.npy',allow_pickle=True))\n",
    "valid_fold_labels_predicted_gmm = list(np.load(data_dir + '50_50_split/valid_fold_labels_predicted_gmm.npy',allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.prediction_strength import calculate_prediction_strength_per_k\n",
    "k_predictions_strength_cv_gmm, k_valid_fold_labels_predicted_gmm, k_valid_fold_labels_gmm, valid_cluster_size_gmm, valid_cluster_size_predicted_gmm = calculate_prediction_strength_per_k(predictions_strengths_cv_gmm,valid_fold_labels_gmm,valid_fold_labels_predicted_gmm, strength_sorted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_k_clusters = [8,9]\n",
    "counter = 0\n",
    "threshold = 0.8\n",
    "n_low_ps_bursts_per_fold = [100,100]\n",
    "n_folds = 2\n",
    "\n",
    "n_init=100\n",
    "max_iter=100\n",
    "\n",
    "\n",
    "sub_dir = '50_50_split/denoising/GMM/k_init=[8,9]_denoising_per_sample/threshold=0.8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.Spectral_clustering_pipeline import spectral_clustering\n",
    "from ipynb.fs.defs.prediction_strength import cross_valdation_prediction_strength\n",
    "from ipynb.fs.defs.prediction_strength import get_low_individual_ps_bursts\n",
    "from ipynb.fs.defs.prediction_strength import get_low_and_high_ps_bursts_fold_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_individual_ps_bursts(data, train_folds, valid_folds, train_fold_labels, valid_fold_labels ,predictions_strengths_cv_per_samples, n_clusters = range(1,21), threshold = 0.8):\n",
    "    \"\"\" extract burst indices for burst with low individual prediction strength for clustering with k clusters per cluster\n",
    "    Args:\n",
    "        data (nd.array): Array containing data (n x m)\n",
    "        train_folds (list of nd.arrays): list of k training set indices each with dimension n-(n/k) x m\n",
    "        valid_folds (list of nd.arrays): list of k validation set indices each with dimension n/k x m\n",
    "        train_fold_labels (list of lists): list of lists containing the cluster labels for each point in training set\n",
    "        valid_fold_labels (list of lists): list of lists containing the cluster labels for each point in validation set\n",
    "        predictions_strengths_cv_per_samples(list of lists): list of lists containin the prediction strength for individual bursts in each cluster by clustering with k clusters for each folds \n",
    "                                                             1. dim n-folds 2.dim n-clusters 3.dim prediction strength for each cluster i in clustering with k clusters \n",
    "        n_clusters (nd.array): range of clusters to use for clustering\n",
    "        n_folds (int): number of folds the data is splitted \n",
    "        threshold (float): cutoff for defining low individual prediction strength                                                    \n",
    "    \n",
    "    Returns:\n",
    "        k_low_individual_ps_cv (list of lists): list of lists containin the burst indices for bursts with individual ps below threshold  strength for individual bursts in each cluster by clustering with k clusters for each folds \n",
    "                                                keys = n_clusters  values = k_folds x n_clusters (burst indices of bursts from validation set with individual ps below threshold for each cluster) \n",
    "    \n",
    "    \"\"\"     \n",
    "    \n",
    "    \n",
    "    k_low_individual_ps_cv = {}\n",
    "    k_low_individual_ps_cv_sizes = {}\n",
    "    k_low_individual_ps_cv_sizes_prop = {}\n",
    "    \n",
    "    for i in n_clusters: # for each clustering ranging from 1 to max n_clusters \n",
    "        k_low_individual_ps_cv[i] = []\n",
    "        k_low_individual_ps_cv_sizes[i] = []\n",
    "        k_low_individual_ps_cv_sizes_prop[i] = []\n",
    "    \n",
    "    for f, fold in enumerate(predictions_strengths_cv_per_samples): # for each fold\n",
    "        #print(f,len(train_fold_labels))\n",
    "        train_fold = train_folds[f] # training set for fold k splitting\n",
    "        valid_fold = valid_folds[f] # validation set for fold k splitting\n",
    "        \n",
    "        train_labels = train_fold_labels[f] # labels for training set  \n",
    "        valid_labels = valid_fold_labels[f] # labels for validation set \n",
    "        \n",
    "        for j,k in enumerate(fold): # for each clustering j with k clusters prediction strenght of fold \n",
    "            train_labels_k = train_labels[j] \n",
    "            valid_labels_k = valid_labels[j]\n",
    "            \n",
    "            k_low_individual_ps_cv_k = []\n",
    "            k_low_individual_ps_cv_sizes_k = []\n",
    "            k_low_individual_ps_cv_sizes_prop_k = []\n",
    "            \n",
    "            for i in range(j+1):\n",
    "                index_in_class = np.where(np.asarray(predictions_strengths_cv_per_samples[f][j][i]) < threshold) #get position relative to class \n",
    "                low_predictive_bursts = valid_fold[np.where(valid_labels_k == i)[0][index_in_class]]\n",
    "                \n",
    "                k_low_individual_ps_cv_k.append(low_predictive_bursts) #get burst indices relative to overall data\n",
    "                k_low_individual_ps_cv_sizes_k.append(len(low_predictive_bursts))\n",
    "                k_low_individual_ps_cv_sizes_prop_k.append(len(low_predictive_bursts)/len(valid_fold[np.where(valid_labels_k == i)[0]]))\n",
    "            \n",
    "            k_low_individual_ps_cv[j+1].append(k_low_individual_ps_cv_k)\n",
    "            k_low_individual_ps_cv_sizes[j+1].append(k_low_individual_ps_cv_sizes_k)\n",
    "            k_low_individual_ps_cv_sizes_prop[j+1].append(k_low_individual_ps_cv_sizes_prop_k)\n",
    "            \n",
    "            \n",
    "            \n",
    "    return  k_low_individual_ps_cv,k_low_individual_ps_cv_sizes,k_low_individual_ps_cv_sizes_prop        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look [8 9] Clusters in each fold!\n",
      "[105 96] Bursts found in each fold with Prediction Strength below threshold = 0.80!\n",
      "Save Number of Bursts with low Prediction Strength for each fold!\n",
      "Fitting GMM with 1 clusters:\n"
     ]
    }
   ],
   "source": [
    "k_clusters = np.asarray(init_k_clusters)\n",
    "threshold = 0.8\n",
    "while True:\n",
    "    print(\"Look [%d %d] Clusters in each fold!\" % tuple(list(k_clusters)))\n",
    "    np.save(data_dir + sub_dir + 'k_clusters_per_fold_%d' % counter , k_clusters)\n",
    "    \n",
    "    if counter == 0:\n",
    "        k_low_individual_ps_bursts, k_low_individual_ps_cv_sizes,k_low_individual_ps_cv_sizes_prop = get_low_individual_ps_bursts(data,train_folds, valid_folds, train_fold_labels_gmm, valid_fold_labels_gmm,predictions_strengths_cv_gmm_per_sample,threshold = threshold)\n",
    "        n_low_ps_bursts_per_fold = [np.sum(k_low_individual_ps_cv_sizes[k_clusters[i]][i]) for i in range(n_folds)] # get low burst examples for each fold after clustering with k_clusters   \n",
    "        \n",
    "        k_high_ps_bursts_folds, high_ps_bursts_fold_labels, k_low_ps_bursts_folds, low_ps_bursts_fold_labels = get_low_and_high_ps_bursts_fold_with_labels(valid_folds,valid_fold_labels_gmm,k_low_individual_ps_bursts,k_low_individual_ps_cv_sizes, n_folds = n_folds,n_clusters = range(1,max(k_clusters)+1))\n",
    "        \n",
    "    else: \n",
    "        k_low_individual_ps_bursts, k_low_individual_ps_cv_sizes,k_low_individual_ps_cv_sizes_prop = get_low_individual_ps_bursts(data,high_ps_train_folds, high_ps_valid_folds, high_ps_train_fold_labels, high_ps_valid_fold_labels,predictions_strengths_cv_per_sample_without_low_samples,threshold = threshold)\n",
    "        n_low_ps_bursts_per_fold = [np.sum(k_low_individual_ps_cv_sizes[k_clusters[i]][i]) for i in range(n_folds)] # get low burst examples for each fold after clustering with k_clusters   \n",
    "        \n",
    "        k_high_ps_bursts_folds, high_ps_bursts_fold_labels, k_low_ps_bursts_folds, low_ps_bursts_fold_labels = get_low_and_high_ps_bursts_fold_with_labels(high_ps_valid_folds,high_ps_valid_fold_labels,k_low_individual_ps_bursts,k_low_individual_ps_cv_sizes, n_folds = n_folds,n_clusters = range(1,max(k_clusters)+1))\n",
    "    \n",
    "    \n",
    "    print(\"[%d %d] Bursts found in each fold with Prediction Strength below threshold = %.2f!\" % (tuple(list(n_low_ps_bursts_per_fold) + [threshold])))\n",
    "    \n",
    "    if np.sum(n_low_ps_bursts_per_fold) == 0:\n",
    "        print(\"Converged!\")\n",
    "        break\n",
    "        \n",
    "    print(\"Save Number of Bursts with low Prediction Strength for each fold!\")\n",
    "    np.save(data_dir + sub_dir + 'n_low_ps_bursts_per_fold_%d' % counter , n_low_ps_bursts_per_fold)\n",
    "    \n",
    "     \n",
    "    high_ps_valid_folds= [] #indices of bursts in validation set with high prediction strenght per fold\n",
    "    new_k_clusters = []\n",
    "    \n",
    "    for i in range(n_folds):  \n",
    "        high_ps_valid_folds.append(np.asarray(k_high_ps_bursts_folds[k_clusters[i]][i])) # get bursts indices with high ps for clustering with k_clusters \n",
    "        new_k_clusters.append(len(np.unique(high_ps_bursts_fold_labels[i][k_clusters[i]-1]))) #get classes with at least one burst \n",
    "    \n",
    "    high_ps_valid_folds = np.asarray(high_ps_valid_folds)\n",
    "    \n",
    "    \n",
    "    clusters = list(range(1,np.amax(new_k_clusters) + 1))\n",
    "        \n",
    "    high_ps_train_folds=[]\n",
    "    high_ps_train_fold_labels=[]\n",
    "    high_ps_valid_fold_labels=[]\n",
    "        \n",
    "    for i in range(n_folds):\n",
    "        high_ps_bursts_train_i = np.concatenate((high_ps_valid_folds[:i],high_ps_valid_folds[(i+1):]), axis = 0)[0]\n",
    "        high_ps_train_folds.append(np.asarray(high_ps_bursts_train_i))\n",
    "            \n",
    "        high_ps_bursts_valid_i = high_ps_valid_folds[i]\n",
    "        high_ps_train_labels_i = np.empty((np.amax(new_k_clusters),), dtype=object)\n",
    "        high_ps_valid_labels_i = np.empty((np.amax(new_k_clusters),), dtype=object)\n",
    "        \n",
    "        for i,c in enumerate(clusters): \n",
    "            print(\"Fitting GMM with %d clusters:\" % c)\n",
    "            \n",
    "            gmm= GMM(c, n_init=n_init, max_iter=max_iter).fit(data[high_ps_bursts_train_i]) \n",
    "            gmm_valid = GMM(c, n_init=n_init, max_iter=max_iter).fit(data[high_ps_bursts_valid_i]) \n",
    "            \n",
    "            \n",
    "            high_ps_train_labels_i[i] = np.asarray(gmm.predict(data[high_ps_bursts_train_i]))\n",
    "            high_ps_valid_labels_i[i] = np.asarray(gmm_valid.predict(data[high_ps_bursts_valid_i]))\n",
    "        \n",
    "        high_ps_train_fold_labels.append(high_ps_train_labels_i)   \n",
    "        high_ps_valid_fold_labels.append(high_ps_valid_labels_i)\n",
    "        \n",
    "   \n",
    "    high_ps_train_folds = np.asarray(high_ps_train_folds)\n",
    "        \n",
    "    \n",
    "    print(\"Save denoised folds!\")\n",
    "    np.save(data_dir + sub_dir + 'high_ps_valid_folds_%d' % (counter + 1), high_ps_valid_folds)\n",
    "    #np.save(data_dir + sub_dir + 'high_ps_train_folds_%d' % (counter + 1), high_ps_train_folds) #not necessary can be derived from validation folds \n",
    "    \n",
    "    print(\"Save labels!\")\n",
    "    np.save(data_dir + sub_dir + 'high_ps_train_fold_labels_%d' % (counter + 1), high_ps_train_fold_labels)\n",
    "    np.save(data_dir + sub_dir + 'high_ps_valid_fold_labels_%d' % (counter + 1), high_ps_valid_fold_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Calculate Prediction Strength per sample for each fold! \")  \n",
    "    predictions_strengths_cv_per_sample_without_low_samples, _ = cross_valdation_prediction_strength(data, high_ps_train_folds, high_ps_valid_folds, high_ps_train_fold_labels, high_ps_valid_fold_labels, per_sample = True)\n",
    "    print(\"Done!\")   \n",
    "    \n",
    "    print(\"Save Prediction Strength!\")\n",
    "    np.save(data_dir + sub_dir + 'predictions_strengths_cv_per_sample_without_low_samples_%d' % (counter + 1), predictions_strengths_cv_per_sample_without_low_samples)\n",
    " \n",
    "    \n",
    "    k_clusters = new_k_clusters\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (7,6441) into shape (7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-73e4a1df8327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msub_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'high_ps_valid_fold_labels_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_ps_valid_fold_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m    509\u001b[0m                            pickle_kwargs=pickle_kwargs)\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (7,6441) into shape (7)"
     ]
    }
   ],
   "source": [
    "np.save(data_dir + sub_dir + 'high_ps_valid_fold_labels_%d' % (counter + 1), high_ps_valid_fold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (7,6450) into shape (7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e7309cc99b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msub_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_ps_train_fold_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m    509\u001b[0m                            pickle_kwargs=pickle_kwargs)\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (7,6450) into shape (7)"
     ]
    }
   ],
   "source": [
    "np.save(data_dir + sub_dir + 'test', high_ps_train_fold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_labels = list(np.load(data_dir + '50_50_split/denoising/spectral_clustering/k_init=11_denoising_per_sample/threshold=0.5/' + 'high_ps_train_fold_labels_3.npy' ,allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_dir + sub_dir + 'test', spectral_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       "       array([0, 0, 0, ..., 0, 0, 0], dtype=int32)], dtype=object)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6480,)\n",
      "(6480,)\n",
      "(6480,)\n",
      "(6480,)\n",
      "(6480,)\n",
      "(6480,)\n",
      "(6480,)\n",
      "(6480,)\n",
      "(6480,)\n",
      "(6480,)\n"
     ]
    }
   ],
   "source": [
    "for i in spectral_labels[0]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6450)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_ps_train_fold_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [3, 0, 3, ..., 0, 3, 3],\n",
       "       [5, 0, 5, ..., 0, 5, 5],\n",
       "       [6, 0, 6, ..., 0, 6, 6]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_ps_train_fold_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  np.empty((7,), dtype=object)\n",
    "\n",
    "for i,j in enumerate(high_ps_train_fold_labels[0]):\n",
    "    test[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
