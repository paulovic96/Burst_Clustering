{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] ='40'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import struct\n",
    "import json\n",
    "import sys\n",
    "import seaborn as sns \n",
    "from scipy import signal, stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dist_matrix(data, metric):\n",
    "    \"\"\" Calculate pairwise distances for each point in dataset with given metric \n",
    "    \n",
    "        Args:\n",
    "            data (nd.array): Array containing data (n x m)\n",
    "            metric (string, or callable): one of sklearns pairwise metrics : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances \n",
    "        \n",
    "        Returns:\n",
    "            dist_matrix (nd.array): matrix of pairwise distances for each datapoint\n",
    "            sorted_dist_matrix (nd.array): indices for row sorting distance matrix\n",
    "    \"\"\"\n",
    "    dist_matrix = pairwise_distances(data, data, metric = metric) # calculate pairwise distances\n",
    "    sorted_dist_matrix = np.argsort(dist_matrix, axis=1)\n",
    "        \n",
    "    return dist_matrix, sorted_dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_knn_graph(matrix,sorted_indices,k=10, mutual = False, weighting = None):\n",
    "    \"\"\" Constuct KNN Graph from distance/similarity matrix \n",
    "    \n",
    "    Args:\n",
    "        matrix (nd.array): nxn matrix containing pairwise distances/similarities \n",
    "        sorted_indices (nd.array): nxn matrix of indices to sort rows of distance/dimilarity matrix in descending order \n",
    "        k (int): number of neighbours to take into account\n",
    "        mutual (bool): Wether to construct knn in mutual manner or not. Mutual in sense of: j beeing in k-nearest neighbours of i does not imply that i is in k-nearest neighbours of j. All vertices in a mutual k-NN graph have a degree upper-bounded by k. \n",
    "        weighting (str): indicate wether matrix contains of similarities or distances\n",
    "    \n",
    "    Returns:\n",
    "        A (nd.arrays): Adjacency matrix of the knn-graph\n",
    "    \"\"\"\n",
    "    A = np.zeros(matrix.shape)\n",
    "    if mutual: # knn graph only when among both knn connect\n",
    "        for i, indices in enumerate(sorted_indices):\n",
    "            if weighting == \"similarity\":\n",
    "                k_nearest = indices[-(k+1) : -1]\n",
    "            else:\n",
    "                k_nearest = indices[1:k+1]    \n",
    "            for j in k_nearest: \n",
    "                if i in sorted_indices[j,1:k+1]:\n",
    "                    if weighting:\n",
    "                        A[i,j] = matrix[i,j]\n",
    "                    else:\n",
    "                        A[i,j] = 1\n",
    "    else:\n",
    "        for i,indices in enumerate(sorted_indices): \n",
    "            if weighting == \"similarity\":\n",
    "                k_nearest = indices[-(k+1) : -1]\n",
    "                A[i,k_nearest] = matrix[i,k_nearest]\n",
    "                A[k_nearest,i] = matrix[k_nearest,i]\n",
    "            else:\n",
    "                k_nearest = indices[1:k+1]\n",
    "            \n",
    "            if weighting==\"distance\":\n",
    "                A[i,k_nearest] = matrix[i,k_nearest]\n",
    "                A[k_nearest,i] = matrix[k_nearest,i]\n",
    "            else:\n",
    "                A[i,k_nearest] = 1\n",
    "                A[k_nearest, i] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_laplacian(A, normalize = True, reg_lambda = 0.1 , saving = False, saving_name = \"L_norm\"): \n",
    "    \"\"\" Calculate the normalized graph Laplacian for given KNN-Graph\n",
    "    \n",
    "    Args:\n",
    "        A (nd.array): Adjacency Matrix of a knn-Graph\n",
    "        normalize(bool): Wether to normalize Laplacian \n",
    "        reg_lambda (int): hyperparameter for regularization strength\n",
    "        saving (bool): True if you want to save matrices for each folds \n",
    "        saving_name (str): File name for saving\n",
    "    Returns:\n",
    "        L (nd.arrays): (normalized) graph Laplacian\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    print(\"Calculate Normalized Laplacians\")\n",
    "\n",
    "    # calcualte normalized Laplacian  \n",
    "    n = A.shape[0] # get number of data points in KNN-Graph\n",
    "    if reg_lambda:\n",
    "        A = A + (reg_lambda/n * np.ones((n,n))) # apply regularization [Zhang and Rohe, 2018]\n",
    "\n",
    "    D = np.sum(A,axis = 1) # get vertices degree \n",
    "    D_inv_sqrt = np.reciprocal(np.sqrt(D))\n",
    "    D_inv_sqrt[np.where(np.isinf(D_inv_sqrt))] = 0 #division by zero \n",
    "    D = np.diag(D)\n",
    "    D_inv_sqrt = np.diag(D_inv_sqrt)\n",
    "\n",
    "    if normalize:\n",
    "        L = D_inv_sqrt @ (D-A) @ D_inv_sqrt # calculate normalized laplacian [Ng et al. 2002] \n",
    "    else:\n",
    "        L = D - A\n",
    "\n",
    "        \n",
    "    if saving: \n",
    "        np.save(home_dir + saving_name,L)\n",
    "        \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eigenvectors_and_values(L, saving = False, saving_name_eigenvectors= \"eigvec\", saving_name_eigenvalues= \"eigval\"):\n",
    "    \"\"\" Calculate the eigenvectors and eigenvalues graph Laplacian\n",
    "    \n",
    "    Args:\n",
    "        L (nd.arrays): (normalized) graph Laplacian\n",
    "        saving (bool): True if you want to save matrices for each folds \n",
    "        saving_name_eigenvectors (str): File name for saving\n",
    "        saving_name_eigenvalues (str): File name for saving\n",
    "\n",
    "    Returns:\n",
    "        eigvec (nd.arrays): eigenvectors of graph Laplacian\n",
    "        eigval (nd.arrays): eigenvalues of graph Laplacian \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Calculate Eigenvalues and Vectors of Laplacian\")\n",
    "\n",
    "    # calcualte eigenvalues and eigenvector \n",
    "    eigval, eigvec  = scipy.linalg.eigh(L)\n",
    "    idx = eigval.argsort()#[::-1] # sort eigenvalues and corresponding eigenvectors in ascending order   \n",
    "\n",
    "    eigval = eigval[idx]\n",
    "    eigvec = eigvec[:,idx]\n",
    "\n",
    "        \n",
    "    if saving: \n",
    "        np.save(home_dir + saving_name_eigenvalues, eigval)\n",
    "        np.save(home_dir + saving_name_eigenvectors, eigvec)\n",
    "\n",
    "\n",
    "    return eigvec, eigval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_eigenvector_embedding(eigenvec, n_cluster):\n",
    "    \"\"\" Cluster eigenvector embedding\n",
    "    \n",
    "    Args:\n",
    "        eigenvec (nd.arrays): Eigenvectors of Graph Laplacian \n",
    "        n_cluster (int): number of clusters \n",
    "\n",
    "    Returns:\n",
    "        labels (list): list of cluster labels for each point \n",
    "    \"\"\"\n",
    "\n",
    "    U = eigenvec[:,:n_cluster] # take first n_cluster eigenvectors into account building a matrix of n X n_clusters \n",
    "    U = U.astype(\"float\")\n",
    "    T = sklearn.preprocessing.normalize(U, norm='l2') # row normalize matrix \n",
    "    \n",
    "    X = T\n",
    "    kmeans = KMeans(n_clusters=n_cluster).fit(X) # apply k-means to cluster eigenvector embedding\n",
    "    labels = kmeans.labels_\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(data, metric, n_clusters,  k=5, mutual = False, weighting = None, normalize = True, reg_lambda = 0.1, save_laplacian = False, save_eigenvalues_and_vectors = False):\n",
    "    \"\"\" Cluster data into n_clusters using spectral clustering  based on eigenvectors of knn-graph laplacian \n",
    "    \n",
    "    Args:\n",
    "        data (nd.array): Array containing data (n x m)\n",
    "        metric (string, or callable): one of sklearns pairwise metrics : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances \n",
    "        n_cluster (list): list of number of n_clusters\n",
    "       \n",
    "        k (int): number of neighbours to take into account\n",
    "        mutual (bool): Wether to construct knn in mutual manner or not. Mutual in sense of: j beeing in k-nearest neighbours of i does not imply that i is in k-nearest neighbours of j. All vertices in a mutual k-NN graph have a degree upper-bounded by k. \n",
    "        weighting (str): indicate wether matrix contains of similarities or distances\n",
    "\n",
    "        normalize(bool): Wether to normalize Laplacian \n",
    "        reg_lambda (int): hyperparameter for regularization strength\n",
    "\n",
    "        save_laplacian (bool): True if you want to save Laplacian  \n",
    "        save_eigenvalues_and_vectors (bool): True if you want to save eigenvectors and eigenvalues  \n",
    "        save_labels (bool): True if you want to save labels \n",
    "\n",
    "    Returns:\n",
    "        labels_per_n_clusters (lists of list): list of lists containing the cluster labels for each point in data set\n",
    "    \"\"\"\n",
    "\n",
    "    dist_matrix, sorted_dist_matrix = calculate_dist_matrix(data, metric)\n",
    "\n",
    "    A = construct_knn_graph(dist_matrix,sorted_dist_matrix,k=k, mutual = mutual, weighting = weighting)\n",
    "\n",
    "    L = calculate_normalized_laplacian(A, normalize = normalize, reg_lambda = reg_lambda , saving = save_laplacian, saving_name = \"L_norm\")\n",
    "\n",
    "    eigvec, eigval = calculate_eigenvectors_and_values(L, saving = save_eigenvalues_and_vectors, saving_name_eigenvectors= \"eigvec\", saving_name_eigenvalues= \"eigval\")\n",
    "    \n",
    "    labels_per_n_clusters = [] \n",
    "    for n_cluster in n_clusters:\n",
    "        labels = cluster_eigenvector_embedding(eigvec, n_cluster)\n",
    "        labels_per_n_clusters.append(labels)\n",
    "    \n",
    "    return labels_per_n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over channels (Day 20):  (13092, 3410)\n",
      "Averaged over channels (Day 20+21):  (24663, 3410)\n"
     ]
    }
   ],
   "source": [
    "data_burst_by_time = np.load(data_dir + 'data_burst_by_time.npy').T\n",
    "data_burst_by_time_20_21 = np.load(data_dir + 'raw_Data/data_burst_by_time_20_21.npy').T\n",
    "data_burst_by_time_shuffled = (np.random.permutation(data_burst_by_time))\n",
    "print(\"Averaged over channels (Day 20): \", data_burst_by_time.shape)\n",
    "print(\"Averaged over channels (Day 20+21): \", data_burst_by_time_20_21.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Cut:  (13092, 1500)\n",
      "Second Cut:  (13092, 1000)\n"
     ]
    }
   ],
   "source": [
    "dataset_cutted = data_burst_by_time[:,1000:2500] # 1. cut 1000 - 2500 2. cut 1200 - 2200\n",
    "dataset_cutted2 = data_burst_by_time[:,1200:2200]\n",
    "print(\"First Cut: \", dataset_cutted.shape)\n",
    "print(\"Second Cut: \", dataset_cutted2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_burst_by_time_20_21 #data_burst_by_time #dataset_cutted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = np.load(data_dir + \"day_20_21_split/train_folds_day_20.npy\") # culture_balanced/culture_balanced_training_split.npy, 50_50_split/train_folds_50_50.npy \n",
    "valid_folds = np.load(data_dir + \"day_20_21_split/valid_folds_day_21.npy\") # culture_balanced/culture_balanced_validation_split.npy, 50_50_split/valid_folds_50_50.npy\n",
    "if len(train_folds.shape)>1:\n",
    "    training_sets = []\n",
    "    validation_sets = []\n",
    "    for i, split in enumerate(train_folds):\n",
    "        training_sets.append(data[split])\n",
    "        validation_sets.append(data[valid_folds[i]])\n",
    "else:\n",
    "    train_folds = [train_folds]\n",
    "    valid_folds = [valid_folds]\n",
    "    training_sets = [data[train_folds]]  #data_burst_by_time[training_split] # extract training bursts from dataset with indices\n",
    "    validation_sets = [data[valid_folds]]  #data_burst_by_time[test_split] # extract validation bursts from dataset with indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 :\n",
      "13092 Bursts in Training Set equal to 53.08 % of the total data. \n",
      "11571 Bursts in Validation Set equal to 46.92 % of the total data. \n"
     ]
    }
   ],
   "source": [
    "for i, train_set in enumerate(training_sets):\n",
    "    print(\"Split %d :\" % (i+1))\n",
    "    print(\"%d Bursts in Training Set equal to %.2f %% of the total data. \" % (len(train_set), np.round((len(train_set)/len(data)), 4) * 100))\n",
    "    print(\"%d Bursts in Validation Set equal to %.2f %% of the total data. \" % (len(validation_sets[i]), np.round((len(validation_sets[i])/len(data)), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters to look at:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "n_clusters = range(1,21)\n",
    "print(\"Number of clusters to look at: \", [x for x in n_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Spectral Clustering for 1. Training Set\n",
      "Calculate Normalized Laplacians\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n"
     ]
    }
   ],
   "source": [
    "training_sets_labels = []\n",
    "for i,train_set in enumerate(training_sets):  \n",
    "    print(\"Start Spectral Clustering for %d. Training Set\" % (i+1))\n",
    "    training_sets_labels.append(spectral_clustering(train_set, \"euclidean\",n_clusters,  k=5, mutual = False, weighting = \"distance\", normalize = True, reg_lambda = 0.1, save_laplacian = False, save_eigenvalues_and_vectors = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sets_labels = []\n",
    "for valid_set in validation_sets: \n",
    "    print(\"Start Spectral Clustering for %d. Validation Set\" % (i+1))\n",
    "    validation_sets_labels.append(spectral_clustering(valid_set, \"euclidean\",n_clusters,  k=5, mutual = False, weighting = \"distance\", normalize = True, reg_lambda = 0.1, save_laplacian = False, save_eigenvalues_and_vectors = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(data_dir + \"day_20_21_split/training_sets_labels_day_20_euclidean_k=5_up_to_20\", training_sets_labels)\n",
    "#np.save(data_dir + \"day_20_21_split/validation_sets_labels_day_21_euclidean_k=5_up_to_20_clusters\", validation_sets_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
