{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoise Data based on prediction Strength per sample \n",
    "Calculate prediction strength per sample and throw out samples with low prediction strength.\n",
    "Reduce the number of clusters if all samples of one cluster are thrown out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] ='40'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import struct\n",
    "import json\n",
    "import sys\n",
    "import seaborn as sns \n",
    "from scipy import signal, stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.defs.prediction_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over channels:  (13092, 3410)\n",
      "(13092, 3410)\n",
      "(13092, 3410)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../Data/'\n",
    "data_burst_by_time = np.load(data_dir + 'data_burst_by_time.npy').T\n",
    "data_burst_by_time_shuffled = (np.random.permutation(data_burst_by_time))\n",
    "print(\"Averaged over channels: \", data_burst_by_time.shape)\n",
    "print(data_burst_by_time.shape)\n",
    "print(data_burst_by_time_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_burst_by_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = np.load(data_dir + \"50_50_split/train_folds_50_50.npy\") #np.load(data_dir + \"culture_balanced/culture_balanced_training_split.npy\")\n",
    "valid_folds = np.load(data_dir + \"50_50_split/valid_folds_50_50.npy\") #np.load(data_dir + \"culture_balanced/culture_balanced_validation_split.npy\")\n",
    "\n",
    "if len(train_folds.shape)>1:\n",
    "    training_sets = []\n",
    "    validation_sets = []\n",
    "    for i, split in enumerate(train_folds):\n",
    "        training_sets.append(data[split])\n",
    "        validation_sets.append(data[valid_folds[i]])\n",
    "else:\n",
    "    train_folds = [train_folds]\n",
    "    valid_folds = [valid_folds]\n",
    "    training_sets = [data[train_folds]]  #data_burst_by_time[training_split] # extract training bursts from dataset with indices\n",
    "    validation_sets = [data[valid_folds]]  #data_burst_by_time[test_split] # extract validation bursts from dataset with indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 :\n",
      "6546 Bursts in Training Set equal to 50.00 % of the total data. \n",
      "6546 Bursts in Validation Set equal to 50.00 % of the total data. \n",
      "Split 2 :\n",
      "6546 Bursts in Training Set equal to 50.00 % of the total data. \n",
      "6546 Bursts in Validation Set equal to 50.00 % of the total data. \n"
     ]
    }
   ],
   "source": [
    "for i, train_set in enumerate(training_sets):\n",
    "    print(\"Split %d :\" % (i+1))\n",
    "    print(\"%d Bursts in Training Set equal to %.2f %% of the total data. \" % (len(train_set), np.round((len(train_set)/len(data)), 4) * 100))\n",
    "    print(\"%d Bursts in Validation Set equal to %.2f %% of the total data. \" % (len(validation_sets[i]), np.round((len(validation_sets[i])/len(data)), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bics = list(np.load(data_dir + '50_50_split/bics_1.npy',allow_pickle=True)) # contains bic score for training and validation set since 50/50 split one file contains full info about bic \n",
    "scores_1 = list(np.load(data_dir + '50_50_split/scores_1.npy',allow_pickle=True)) # scores for training and validation set \n",
    "scores_2 = list(np.load(data_dir + '50_50_split/scores_1.npy',allow_pickle=True))\n",
    "\n",
    "predictions_valid_1 = list(np.load(data_dir + '50_50_split/predictions_valid_1.npy',allow_pickle=True)) # predictions of validation set \n",
    "predictions_valid_2 = list(np.load(data_dir + '50_50_split/predictions_valid_2.npy',allow_pickle=True))\n",
    "\n",
    "predictions_train_1 = list(np.load(data_dir + '50_50_split/predictions_train_1.npy',allow_pickle=True)) # predictions of training set \n",
    "predictions_train_2 = list(np.load(data_dir + '50_50_split/predictions_train_2.npy',allow_pickle=True)) \n",
    "\n",
    "predictions_valid_fitted_1 = list(np.load(data_dir + '50_50_split/predictions_valid_fitted_1.npy',allow_pickle=True)) # predictions of validation set after fitting gmm on it\n",
    "predictions_valid_fitted_2 = list(np.load(data_dir + '50_50_split/predictions_valid_fitted_2.npy',allow_pickle=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "n_folds = 2\n",
    "\n",
    "\n",
    "train_fold_labels_gmm = np.stack((predictions_train_1,predictions_train_2),axis = 0)\n",
    "valid_fold_labels_gmm = np.stack((predictions_valid_fitted_1,predictions_valid_fitted_2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_strengths_cv_gmm_per_sample = list(np.load(data_dir + '50_50_split/prediction_strength_gmm_per_sample.npy',allow_pickle=True))\n",
    "predictions_strengths_cv_gmm = list(np.load(data_dir + '50_50_split/prediction_strength_gmm.npy',allow_pickle=True))\n",
    "valid_fold_labels_predicted_gmm = list(np.load(data_dir + '50_50_split/valid_fold_labels_predicted_gmm.npy',allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.prediction_strength import calculate_prediction_strength_per_k\n",
    "k_predictions_strength_cv_gmm, k_valid_fold_labels_predicted_gmm, k_valid_fold_labels_gmm, valid_cluster_size_gmm, valid_cluster_size_predicted_gmm = calculate_prediction_strength_per_k(predictions_strengths_cv_gmm,valid_fold_labels_gmm,valid_fold_labels_predicted_gmm, strength_sorted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_k_clusters = [8,9]\n",
    "counter = 0\n",
    "threshold = 0.5\n",
    "n_low_ps_bursts_per_fold = [100,100]\n",
    "n_folds = 2\n",
    "\n",
    "n_init=100\n",
    "max_iter=100\n",
    "\n",
    "\n",
    "sub_dir = '50_50_split/denoising/GMM/k_init=[8,9]_denoising_per_sample/threshold=0.5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.Spectral_clustering_pipeline import spectral_clustering\n",
    "from ipynb.fs.defs.prediction_strength import cross_valdation_prediction_strength\n",
    "from ipynb.fs.defs.prediction_strength import get_low_individual_ps_bursts\n",
    "from ipynb.fs.defs.prediction_strength import get_low_and_high_ps_bursts_fold_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter = 3\n",
    "#k_clusters = list(np.load(data_dir + sub_dir + \"k_clusters_per_fold_3.npy\" ,allow_pickle=True))\n",
    "#high_ps_valid_folds = np.load(data_dir + sub_dir + \"high_ps_valid_folds_3.npy\" ,allow_pickle=True)\n",
    "#high_ps_train_folds=[]\n",
    "#for i in range(n_folds):\n",
    "#        high_ps_bursts_train_i = np.concatenate((high_ps_valid_folds[:i],high_ps_valid_folds[(i+1):]), axis = 0)[0]\n",
    "#        high_ps_train_folds.append(np.asarray(high_ps_bursts_train_i))\n",
    "        \n",
    "#high_ps_train_fold_labels = np.load(data_dir + sub_dir + \"high_ps_train_fold_labels_3.npy\" ,allow_pickle=True)\n",
    "#high_ps_valid_fold_labels = np.load(data_dir + sub_dir + \"high_ps_valid_fold_labels_3.npy\" ,allow_pickle=True)\n",
    "\n",
    "\n",
    "#predictions_strengths_cv_per_sample_without_low_samples = np.load(data_dir + sub_dir + \"predictions_strengths_cv_per_sample_without_low_samples_3.npy\" ,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look [8 9] Clusters in each fold!\n",
      "[46 48] Bursts found in each fold with Prediction Strength below threshold = 0.50!\n",
      "Save Number of Bursts with low Prediction Strength for each fold!\n",
      "Fitting GMM with 1 clusters:\n"
     ]
    }
   ],
   "source": [
    "k_clusters = np.asarray(init_k_clusters)\n",
    "threshold = 0.5\n",
    "while True:\n",
    "    print(\"Look [%d %d] Clusters in each fold!\" % tuple(list(k_clusters)))\n",
    "    np.save(data_dir + sub_dir + 'k_clusters_per_fold_%d' % counter , k_clusters)\n",
    "    \n",
    "    if counter == 0:\n",
    "        k_low_individual_ps_bursts, k_low_individual_ps_cv_sizes,k_low_individual_ps_cv_sizes_prop = get_low_individual_ps_bursts(data,train_folds, valid_folds, train_fold_labels_gmm, valid_fold_labels_gmm,predictions_strengths_cv_gmm_per_sample,threshold = threshold)\n",
    "        n_low_ps_bursts_per_fold = [np.sum(k_low_individual_ps_cv_sizes[k_clusters[i]][i]) for i in range(n_folds)] # get low burst examples for each fold after clustering with k_clusters   \n",
    "        \n",
    "        k_high_ps_bursts_folds, high_ps_bursts_fold_labels, k_low_ps_bursts_folds, low_ps_bursts_fold_labels = get_low_and_high_ps_bursts_fold_with_labels(valid_folds,valid_fold_labels_gmm,k_low_individual_ps_bursts,k_low_individual_ps_cv_sizes, n_folds = n_folds,n_clusters = range(1,max(k_clusters)+1))\n",
    "        \n",
    "    else: \n",
    "        k_low_individual_ps_bursts, k_low_individual_ps_cv_sizes,k_low_individual_ps_cv_sizes_prop = get_low_individual_ps_bursts(data,high_ps_train_folds, high_ps_valid_folds, high_ps_train_fold_labels, high_ps_valid_fold_labels,predictions_strengths_cv_per_sample_without_low_samples,threshold = threshold)\n",
    "        n_low_ps_bursts_per_fold = [np.sum(k_low_individual_ps_cv_sizes[k_clusters[i]][i]) for i in range(n_folds)] # get low burst examples for each fold after clustering with k_clusters   \n",
    "        \n",
    "        k_high_ps_bursts_folds, high_ps_bursts_fold_labels, k_low_ps_bursts_folds, low_ps_bursts_fold_labels = get_low_and_high_ps_bursts_fold_with_labels(high_ps_valid_folds,high_ps_valid_fold_labels,k_low_individual_ps_bursts,k_low_individual_ps_cv_sizes, n_folds = n_folds,n_clusters = range(1,max(k_clusters)+1))\n",
    "    \n",
    "    \n",
    "    print(\"[%d %d] Bursts found in each fold with Prediction Strength below threshold = %.2f!\" % (tuple(list(n_low_ps_bursts_per_fold) + [threshold])))\n",
    "    \n",
    "    if np.sum(n_low_ps_bursts_per_fold) == 0:\n",
    "        print(\"Converged!\")\n",
    "        break\n",
    "        \n",
    "    print(\"Save Number of Bursts with low Prediction Strength for each fold!\")\n",
    "    np.save(data_dir + sub_dir + 'n_low_ps_bursts_per_fold_%d' % counter , n_low_ps_bursts_per_fold)\n",
    "    \n",
    "     \n",
    "    high_ps_valid_folds= [] #indices of bursts in validation set with high prediction strenght per fold\n",
    "    new_k_clusters = []\n",
    "    \n",
    "    for i in range(n_folds):  \n",
    "        high_ps_valid_folds.append(np.asarray(k_high_ps_bursts_folds[k_clusters[i]][i])) # get bursts indices with high ps for clustering with k_clusters \n",
    "        new_k_clusters.append(len(np.unique(high_ps_bursts_fold_labels[i][k_clusters[i]-1]))) #get classes with at least one burst \n",
    "    \n",
    "    high_ps_valid_folds = np.asarray(high_ps_valid_folds)\n",
    "    \n",
    "    \n",
    "    clusters = list(range(1,np.amax(new_k_clusters) + 1))\n",
    "        \n",
    "    high_ps_train_folds=[]\n",
    "    high_ps_train_fold_labels=[]\n",
    "    high_ps_valid_fold_labels=[]\n",
    "        \n",
    "    for i in range(n_folds):\n",
    "        high_ps_bursts_train_i = np.concatenate((high_ps_valid_folds[:i],high_ps_valid_folds[(i+1):]), axis = 0)[0]\n",
    "        high_ps_train_folds.append(np.asarray(high_ps_bursts_train_i))\n",
    "            \n",
    "        high_ps_bursts_valid_i = high_ps_valid_folds[i]\n",
    "        high_ps_train_labels_i = np.empty((np.amax(new_k_clusters),), dtype=object)\n",
    "        high_ps_valid_labels_i = np.empty((np.amax(new_k_clusters),), dtype=object)\n",
    "        \n",
    "        for i,c in enumerate(clusters): \n",
    "            print(\"Fitting GMM with %d clusters:\" % c)\n",
    "            \n",
    "            gmm= GMM(c, n_init=n_init, max_iter=max_iter).fit(data[high_ps_bursts_train_i]) \n",
    "            gmm_valid = GMM(c, n_init=n_init, max_iter=max_iter).fit(data[high_ps_bursts_valid_i]) \n",
    "            \n",
    "            \n",
    "            high_ps_train_labels_i[i] = np.asarray(gmm.predict(data[high_ps_bursts_train_i]))\n",
    "            high_ps_valid_labels_i[i] = np.asarray(gmm_valid.predict(data[high_ps_bursts_valid_i]))\n",
    "        \n",
    "        high_ps_train_fold_labels.append(high_ps_train_labels_i)   \n",
    "        high_ps_valid_fold_labels.append(high_ps_valid_labels_i)\n",
    "        \n",
    "   \n",
    "    high_ps_train_folds = np.asarray(high_ps_train_folds)\n",
    "        \n",
    "    \n",
    "    print(\"Save denoised folds!\")\n",
    "    np.save(data_dir + sub_dir + 'high_ps_valid_folds_%d' % (counter + 1), high_ps_valid_folds)\n",
    "    #np.save(data_dir + sub_dir + 'high_ps_train_folds_%d' % (counter + 1), high_ps_train_folds) #not necessary can be derived from validation folds \n",
    "    \n",
    "    print(\"Save labels!\")\n",
    "    np.save(data_dir + sub_dir + 'high_ps_train_fold_labels_%d' % (counter + 1), high_ps_train_fold_labels)\n",
    "    np.save(data_dir + sub_dir + 'high_ps_valid_fold_labels_%d' % (counter + 1), high_ps_valid_fold_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Calculate Prediction Strength per sample for each fold! \")  \n",
    "    predictions_strengths_cv_per_sample_without_low_samples, _ = cross_valdation_prediction_strength(data, high_ps_train_folds, high_ps_valid_folds, high_ps_train_fold_labels, high_ps_valid_fold_labels, per_sample = True)\n",
    "    print(\"Done!\")   \n",
    "    \n",
    "    print(\"Save Prediction Strength!\")\n",
    "    np.save(data_dir + sub_dir + 'predictions_strengths_cv_per_sample_without_low_samples_%d' % (counter + 1), predictions_strengths_cv_per_sample_without_low_samples)\n",
    " \n",
    "    \n",
    "    k_clusters = new_k_clusters\n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
