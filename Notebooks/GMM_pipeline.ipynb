{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] ='40'\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import struct\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import seaborn as sns \n",
    "from scipy import signal, stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture as GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "blis_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/opt/conda/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "blas_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/opt/conda/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_lapack_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/opt/conda/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/opt/conda/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n"
     ]
    }
   ],
   "source": [
    "np.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Data/'\n",
    "sub_dir = '50_50_split/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_gmm_models(X_train_sets, X_valid_sets, clusters,n_folds = 1, n_init=100, max_iter=100, init_from_checkpoint = False):\n",
    "    bics_per_fold = []\n",
    "    scores_per_fold = []\n",
    "    predictions_train_per_fold =[] \n",
    "    predictions_valid_per_fold = [] \n",
    "    predictions_valid_fitted_per_fold = []\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        print(\"Start Process for %d. split!\" %(i+1))\n",
    "        if init_from_checkpoint:\n",
    "            print(\"Initialize from stored Checkpoint!\")\n",
    "            bics = list(np.load(data_dir + sub_dir + 'bics_%d.npy' % (i+1),allow_pickle=True))\n",
    "            scores = list(np.load(data_dir + sub_dir + 'scores_%d.npy' % (i+1),allow_pickle=True))\n",
    "            predictions_valid = list(np.load(data_dir + sub_dir + 'predictions_valid_%d.npy' % (i+1),allow_pickle=True))\n",
    "            predictions_train = list(np.load(data_dir + sub_dir + 'predictions_train_%d.npy' % (i+1),allow_pickle=True))\n",
    "            predictions_valid_fitted = list(np.load(data_dir + sub_dir + 'predictions_valid_fitted_%d.npy' % (i+1),allow_pickle=True))\n",
    "\n",
    "        else:\n",
    "            bics = [] # for each fold and each cluster \n",
    "            scores = [] # for each fold and each cluster\n",
    "            predictions_valid = []\n",
    "            predictions_train = []\n",
    "            predictions_valid_fitted = []\n",
    "        \n",
    "        X_train = X_train_sets[i]\n",
    "        X_valid = X_valid_sets[i]\n",
    "        \n",
    "        for c in clusters:\n",
    "            print(\"Fitting GMM with %d clusters:\" % c)\n",
    "\n",
    "            gmm= GMM(c, n_init=n_init, max_iter=max_iter).fit(X_train) \n",
    "            gmm_valid = GMM(c, n_init=n_init, max_iter=max_iter).fit(X_valid) \n",
    "\n",
    "            bics.append((gmm.bic(X_train),gmm_valid.bic(X_valid)))\n",
    "            scores.append((gmm.score(X_train),gmm.score(X_valid)))\n",
    "\n",
    "            print(\"Predicting Data Set!\")\n",
    "            predictions_train.append(gmm.predict(X_train))\n",
    "\n",
    "            print(\"Predicting Validation Set!\")\n",
    "            predictions_valid.append(gmm.predict(X_valid))\n",
    "\n",
    "            print(\"Prediction Vlaidation Set after fitting separately!\")\n",
    "            predictions_valid_fitted.append(gmm_valid.predict(X_valid))\n",
    "\n",
    "            np.save(data_dir + sub_dir + 'bics_%d.npy' % (i+1),bics) \n",
    "            np.save(data_dir + sub_dir + 'scores_%d.npy' % (i+1),scores)\n",
    "            np.save(data_dir + sub_dir + 'predictions_valid_%d.npy' % (i+1),predictions_valid)\n",
    "            np.save(data_dir + sub_dir + 'predictions_train_%d.npy' % (i+1),predictions_train)\n",
    "            np.save(data_dir + sub_dir + 'predictions_valid_fitted_%d.npy' % (i+1),predictions_valid_fitted)\n",
    "\n",
    "            print(\"Saved GMM data with %d clusters!\" % c)\n",
    "        \n",
    "        bics_per_fold.append(bics)\n",
    "        scores_per_fold.append(scores)\n",
    "        predictions_train_per_fold.append(predictions_train)\n",
    "        predictions_valid_per_fold.append(predictions_valid)\n",
    "        predictions_valid_fitted_per_fold.append(predictions_valid_fitted)\n",
    "\n",
    "    return bics_per_fold, scores_per_fold, predictions_train_per_fold, predictions_valid_per_fold, predictions_valid_fitted_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over channels:  (13092, 3410)\n",
      "(13092, 3410)\n",
      "(13092, 3410)\n"
     ]
    }
   ],
   "source": [
    "data_burst_by_time = np.load(data_dir + 'data_burst_by_time.npy').T\n",
    "data_burst_by_time_shuffled = (np.random.permutation(data_burst_by_time))\n",
    "print(\"Averaged over channels: \", data_burst_by_time.shape)\n",
    "print(data_burst_by_time.shape)\n",
    "print(data_burst_by_time_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Cut:  (13092, 1500)\n",
      "Second Cut:  (13092, 1000)\n"
     ]
    }
   ],
   "source": [
    "dataset_cutted = data_burst_by_time[:,1000:2500] # 1. cut 1000 - 2500 2. cut 1200 - 2200\n",
    "dataset_cutted2 = data_burst_by_time[:,1200:2200]\n",
    "print(\"First Cut: \", dataset_cutted.shape)\n",
    "print(\"Second Cut: \", dataset_cutted2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_burst_by_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = np.load(data_dir + sub_dir +  \"train_folds_50_50.npy\")#np.load(data_dir + \"culture_balanced/culture_balanced_training_split.npy\")\n",
    "valid_folds = np.load(data_dir + sub_dir + \"valid_folds_50_50.npy\")#np.load(data_dir + \"culture_balanced/culture_balanced_validation_split.npy\")\n",
    "\n",
    "if len(train_folds.shape)>1:\n",
    "    training_sets = []\n",
    "    validation_sets = []\n",
    "    for i, split in enumerate(train_folds):\n",
    "        training_sets.append(data[split])\n",
    "        validation_sets.append(data[valid_folds[i]])\n",
    "else:\n",
    "    train_folds = [train_folds]\n",
    "    valid_folds = [valid_folds]\n",
    "    training_sets = [data[train_folds]]  #data_burst_by_time[training_split] # extract training bursts from dataset with indices\n",
    "    validation_sets = [data[valid_folds]]  #data_burst_by_time[test_split] # extract validation bursts from dataset with indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 :\n",
      "6546 Bursts in Training Set equal to 50.00 % of the total data. \n",
      "6546 Bursts in Validation Set equal to 50.00 % of the total data. \n",
      "Split 2 :\n",
      "6546 Bursts in Training Set equal to 50.00 % of the total data. \n",
      "6546 Bursts in Validation Set equal to 50.00 % of the total data. \n"
     ]
    }
   ],
   "source": [
    "for i, train_set in enumerate(training_sets):\n",
    "    print(\"Split %d :\" % (i+1))\n",
    "    print(\"%d Bursts in Training Set equal to %.2f %% of the total data. \" % (len(train_set), np.round((len(train_set)/len(data)), 4) * 100))\n",
    "    print(\"%d Bursts in Validation Set equal to %.2f %% of the total data. \" % (len(validation_sets[i]), np.round((len(validation_sets[i])/len(data)), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters to look at:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "n_clusters = range(1,21)\n",
    "print(\"Number of clusters to look at: \", [x for x in n_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "test1 = [training_sets[0][0:2]]\n",
    "test2 = [validation_sets[0][0:2]]\n",
    "print(len(test1))\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Process for 1. split!\n",
      "Fitting GMM with 1 clusters:\n",
      "Predicting Data Set!\n",
      "Predicting Validation Set!\n",
      "Prediction Vlaidation Set after fitting separately!\n",
      "Saved GMM data with 1 clusters!\n",
      "Fitting GMM with 2 clusters:\n",
      "Predicting Data Set!\n",
      "Predicting Validation Set!\n",
      "Prediction Vlaidation Set after fitting separately!\n",
      "Saved GMM data with 2 clusters!\n",
      "Fitting GMM with 3 clusters:\n",
      "Predicting Data Set!\n",
      "Predicting Validation Set!\n",
      "Prediction Vlaidation Set after fitting separately!\n",
      "Saved GMM data with 3 clusters!\n",
      "Fitting GMM with 4 clusters:\n",
      "Predicting Data Set!\n",
      "Predicting Validation Set!\n",
      "Prediction Vlaidation Set after fitting separately!\n",
      "Saved GMM data with 4 clusters!\n",
      "Fitting GMM with 5 clusters:\n",
      "Predicting Data Set!\n",
      "Predicting Validation Set!\n",
      "Prediction Vlaidation Set after fitting separately!\n",
      "Saved GMM data with 5 clusters!\n",
      "Fitting GMM with 6 clusters:\n",
      "Predicting Data Set!\n",
      "Predicting Validation Set!\n",
      "Prediction Vlaidation Set after fitting separately!\n",
      "Saved GMM data with 6 clusters!\n",
      "Fitting GMM with 7 clusters:\n"
     ]
    }
   ],
   "source": [
    "bics_per_fold, scores_per_fold, predictions_train_per_fold, predictions_valid_per_fold, predictions_valid_fitted_per_fold = fit_and_validate_gmm_models(training_sets, validation_sets, n_clusters,n_folds=2, n_init=100, max_iter=100, init_from_checkpoint = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
