{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paule/anaconda3/envs/bon17/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.nearest_centroid module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from spectral_clustering import spectral_clustering\n",
    "import functions_for_plotting\n",
    "from asymmetric_laplacian_distribution import get_index_per_class, get_labels, labels_to_layout_mapping\n",
    "from sklearn.cluster import KMeans\n",
    "import training_set_split\n",
    "import seaborn as sns\n",
    "import prediction_strength\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and True Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------- DATA ------------------------------------------------------------------\n",
    "data_dir = \"data/\"\n",
    "\n",
    "clear_data = np.load(data_dir + \"clearly_separated_data_F_signal_noise.npy\")\n",
    "ambig_data = np.load(data_dir + \"ambiguous_data_tau_amplitude_F_signal_noise.npy\")\n",
    "#np.load(data_dir + \"ambiguous_data_tau_amplitude_F_signal_noise.npy\") #np.load(data_dir + \"clearly_separated_data_F_signal_noise.npy\")\n",
    "\n",
    "clear_amplitude_conditions = [\"S\", \"M\", \"L\"]  #[\"S\", \"S/M\", \"M\", \"M/L\", \"L\"] #[\"S\", \"M\", \"L\"]\n",
    "ambig_amplitude_conditions = [\"S\", \"S/M\", \"M\", \"M/L\", \"L\"]\n",
    "\n",
    "clear_time_constant_conditions = [\"equal_sharp\", \"equal_wide\", \"wide_sharp_negative_skew\", \"sharp_wide_positive_skew\"]\n",
    "ambig_time_constant_conditions = [\"equal_sharp\", \"equal_medium\", \"equal_wide\", \"wide_sharp_negative_skew\", \"wide_medium_negative_skew\",\"medium_sharp_negative_skew\",\"sharp_wide_positive_skew\", \"medium_wide_positive_skew\" ,\"sharp_medium_positive_skew\"]\n",
    "\n",
    "#[\"equal_sharp\", \"equal_medium\", \"equal_wide\", \"wide_sharp_negative_skew\", \"wide_medium_negative_skew\",\"medium_sharp_negative_skew\",\"sharp_wide_positive_skew\", \"medium_wide_positive_skew\" ,\"sharp_medium_positive_skew\"]\n",
    "#[\"equal_sharp\", \"equal_wide\", \"wide_sharp_negative_skew\", \"sharp_wide_positive_skew\"]\n",
    "\n",
    "ambiguous_conditions = [\"S/M\", \"M/L\", \"equal_medium\", \"wide_medium_negative_skew\", \"medium_sharp_negative_skew\", \"medium_wide_positive_skew\", \"sharp_medium_positive_skew\"]\n",
    "\n",
    "samples_per_condition = 1000\n",
    "samples_per_ambiguous_condition = 400\n",
    "\n",
    "ambig_cluster_dict = get_index_per_class(ambig_amplitude_conditions,ambig_time_constant_conditions, ambiguous_conditions, samples_per_condition, samples_per_ambiguous_condition)\n",
    "clear_cluster_dict = get_index_per_class(clear_amplitude_conditions,clear_time_constant_conditions, [], samples_per_condition, samples_per_ambiguous_condition)\n",
    "\n",
    "\n",
    "clear_true_labels = get_labels(clear_data, clear_cluster_dict)\n",
    "ambig_true_labels = get_labels(ambig_data, ambig_cluster_dict)\n",
    "\n",
    "clear_clusters_ordered = list(range(0,len(clear_cluster_dict)+1))\n",
    "clear_layout_label_mapping = labels_to_layout_mapping(clear_clusters_ordered, 4, (1,4)) #labels_to_layout_mapping(clusters_ordered, 4, (1,4)) #labels_to_layout_mapping(clusters_ordered, 9, (2,5))\n",
    "\n",
    "ambig_clusters_ordered = list(range(0,len(ambig_cluster_dict)+1))\n",
    "ambig_layout_label_mapping = labels_to_layout_mapping(ambig_clusters_ordered, 9, (2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Balanced Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_train_fold_indices, _ = training_set_split.get_training_folds(clear_data, clear_cluster_dict,cluster_split=\"balanced\",folds = 2)\n",
    "ambig_train_fold_indices, _ = training_set_split.get_training_folds(ambig_data, ambig_cluster_dict,cluster_split=\"balanced\",folds = 2)\n",
    "\n",
    "\n",
    "clear_training_set = clear_data[clear_train_fold_indices[0]]\n",
    "clear_validation_set = clear_data[clear_train_fold_indices[1]]\n",
    "\n",
    "ambig_training_set = ambig_data[ambig_train_fold_indices[0]]\n",
    "ambig_validation_set = ambig_data[ambig_train_fold_indices[1]]\n",
    "\n",
    "clear_true_labels_training = clear_true_labels[clear_train_fold_indices[0]]\n",
    "clear_true_labels_validation = clear_true_labels[clear_train_fold_indices[1]]\n",
    "\n",
    "ambig_true_labels_training = ambig_true_labels[ambig_train_fold_indices[0]]\n",
    "ambig_true_labels_validation = ambig_true_labels[ambig_train_fold_indices[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Unbalanced Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(42)\n",
    "clear_clusters = np.arange(12)\n",
    "ambig_clusters = np.arange(45)\n",
    "np.random.shuffle(clear_clusters)\n",
    "np.random.shuffle(ambig_clusters)\n",
    "\n",
    "clear_training_clusters = clear_clusters[0:6]\n",
    "ambig_training_clusters = ambig_clusters[0:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_train_fold_indices, clear_valid_fold_indices = training_set_split.get_training_folds(clear_data, clear_cluster_dict,cluster_split=\"unbalanced\",training_clusters = clear_training_clusters,folds = 2)\n",
    "ambig_train_fold_indices, ambig_valid_fold_indices = training_set_split.get_training_folds(ambig_data, ambig_cluster_dict,cluster_split=\"unbalanced\",training_clusters = ambig_training_clusters, folds = 2)\n",
    "\n",
    "\n",
    "clear_training_set = clear_data[clear_train_fold_indices]\n",
    "clear_validation_set = clear_data[clear_valid_fold_indices]\n",
    "\n",
    "ambig_training_set = ambig_data[ambig_train_fold_indices]\n",
    "ambig_validation_set = ambig_data[ambig_train_fold_indices]\n",
    "\n",
    "clear_true_labels_training = clear_true_labels[clear_train_fold_indices]\n",
    "clear_true_labels_validation = clear_true_labels[clear_valid_fold_indices]\n",
    "\n",
    "ambig_true_labels_training = ambig_true_labels[ambig_train_fold_indices]\n",
    "ambig_true_labels_validation = ambig_true_labels[ambig_valid_fold_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.,  2.,  5.,  8.,  9., 10.]), array([1000, 1000, 1000, 1000, 1000, 1000]))\n",
      "(array([ 1.,  3.,  4.,  6.,  7., 11.]), array([1000, 1000, 1000, 1000, 1000, 1000]))\n",
      "(array([ 0.,  3.,  4.,  5.,  6.,  7.,  8., 10., 12., 13., 17., 18., 19.,\n",
      "       22., 25., 30., 31., 34., 35., 36., 38., 42., 44.]), array([1000, 1000,  400,  400, 1000,  400,  400,  400,  400,  400,  400,\n",
      "       1000,  400,  400,  400,  400,  400,  400,  400, 1000, 1000, 1000,\n",
      "        400]))\n",
      "(array([ 1.,  2.,  9., 11., 14., 15., 16., 20., 21., 23., 24., 26., 27.,\n",
      "       28., 29., 32., 33., 37., 39., 40., 41., 43.]), array([ 400, 1000,  400,  400,  400,  400,  400, 1000, 1000,  400, 1000,\n",
      "        400,  400,  400,  400,  400,  400,  400, 1000,  400,  400,  400]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(clear_true_labels_training,return_counts = True))\n",
    "print(np.unique(clear_true_labels_validation,return_counts = True))\n",
    "print(np.unique(ambig_true_labels_training,return_counts = True))\n",
    "print(np.unique(ambig_true_labels_validation,return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = 10\n",
    "#reg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Normalized Laplacians\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n"
     ]
    }
   ],
   "source": [
    "#labels, eigvec, eigval = spectral_clustering(clear_training_set, \"euclidean\", range(1,50),  k=k, mutual = False, weighting = \"distance\", normalize = True, reg_lambda = reg, save_laplacian = False, save_eigenvalues_and_vectors = False)\n",
    "#np.save(\"labels_k=%d_reg=%s_clear_training_unbalanced\" % (k, str(reg)),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Normalized Laplacians\n",
      "Calculate Eigenvalues and Vectors of Laplacian\n"
     ]
    }
   ],
   "source": [
    "#labels, eigvec, eigval = spectral_clustering(clear_validation_set, \"euclidean\", range(1,50),  k=k, mutual = False, weighting = \"distance\", normalize = True, reg_lambda = reg, save_laplacian = False, save_eigenvalues_and_vectors = False)\n",
    "#np.save(\"labels_k=%d_reg=%s_clear_validation_unbalanced\" % (k, str(reg)),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_file_clusters = \"true_clusters_clear_unbalanced_validation_mean.pdf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions_for_plotting.plot_clusters(clear_validation_set, clear_true_labels_validation,clear_true_labels_validation, 3,4, clear_layout_label_mapping,figsize=(20,20),n_bursts = 100,y_lim = (0,16),save_file=save_file_clusters ,subplot_adjustments= [0.05,0.95,0.03,0.9,0.4, 0.15], plot_mean=True, title= \"Validation Set Clusters (Mean)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction strength vs. F1 score \n",
    "- True positives: same cluster in  training, same cluster in validation\n",
    "- False positives: different cluster in training, same cluster in validation\n",
    "- True negatives: different cluster in training, different cluster in validation\n",
    "- False negatives: same cluster in training, different cluster in validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_strength import calculate_centroids_and_predict_validation_data\n",
    "from prediction_strength import get_confusion_matrix\n",
    "from prediction_strength import get_recall\n",
    "from prediction_strength import get_precision\n",
    "from prediction_strength import get_F1_score\n",
    "from math import factorial\n",
    "from itertools import permutations\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1_score_per_k(data, train_indices, valid_indices, train_labels, valid_labels,combination_type = \"full\", true_train_labels = None, own_combinations=None):\n",
    "    k_clusters = list(valid_labels.keys())\n",
    "    training_set = data[train_indices]\n",
    "    validation_set = data[valid_indices]\n",
    "    \n",
    "    if combination_type == \"true\":\n",
    "        print(\"Calculate F1 score based on true training centroids!\")\n",
    "        if not true_train_labels is None:\n",
    "            true_F1_score_per_k = {} \n",
    "            for k in k_clusters:\n",
    "                train_labels_k = true_train_labels\n",
    "                valid_labels_k = valid_labels[k]\n",
    "                \n",
    "                if len(np.unique(train_labels_k))>1:\n",
    "                    centroids_k, labels_centroids_based = calculate_centroids_and_predict_validation_data(training_set,train_labels_k,validation_set)\n",
    "\n",
    "                else:\n",
    "                    labels_centroids_based = np.zeros(len(validation_set))\n",
    "\n",
    "                unique_valid_labels = list(range(k))\n",
    "                unique_centroid_labels = list(np.unique(train_labels_k))\n",
    "\n",
    "                true_positives, false_positives, true_negatives, false_negatives = get_confusion_matrix(valid_labels_k,labels_centroids_based,unique_valid_labels, unique_centroid_labels)\n",
    "                precision = get_precision(true_positives,false_positives)\n",
    "                recall = get_recall(true_positives, false_negatives)\n",
    "                f1 = get_F1_score(recall, precision)\n",
    "\n",
    "                true_F1_score_per_k[k] = f1 \n",
    "\n",
    "            return true_F1_score_per_k\n",
    "        else:\n",
    "            print(\"True labels for the training set not found..Please provide true labels!!\")\n",
    "    \n",
    "    elif combination_type == \"equal\":\n",
    "        print(\"Calculate F1 score based on training centroids assuming same number of clusters in both sets!\")\n",
    "        F1_score_per_k = {}\n",
    "        for k in k_clusters:\n",
    "            train_labels_k = train_labels[k]\n",
    "            valid_labels_k = valid_labels[k]\n",
    "\n",
    "            if k>1:\n",
    "                centroids_k, labels_centroids_based = calculate_centroids_and_predict_validation_data(training_set,train_labels_k,validation_set)\n",
    "\n",
    "            else:\n",
    "                labels_centroids_based = np.zeros(len(validation_set))\n",
    "\n",
    "            unique_valid_labels = list(range(k))\n",
    "            unique_centroid_labels = list(range(k))\n",
    "\n",
    "            true_positives, false_positives, true_negatives, false_negatives = get_confusion_matrix(valid_labels_k,labels_centroids_based,unique_valid_labels, unique_centroid_labels)\n",
    "            precision = get_precision(true_positives,false_positives)\n",
    "            recall = get_recall(true_positives, false_negatives)\n",
    "            f1 = get_F1_score(recall, precision)\n",
    "\n",
    "            F1_score_per_k[k] = f1 \n",
    "\n",
    "        return F1_score_per_k\n",
    "        \n",
    "        \n",
    "    elif combination_type == \"full\":\n",
    "        print(\"Calculate F1 score based on training centroids with full permutation of possible clusters in both sets!\")\n",
    "        F1_score_per_k_combination = {} \n",
    "        counter = 0\n",
    "        for k1k2 in product(k_clusters, repeat=2):\n",
    "            counter += 1\n",
    "            if counter%50 == 0:\n",
    "                print(\"Step:%d\" % counter)\n",
    "            train_labels_k = train_labels[k1k2[0]]\n",
    "            valid_labels_k = valid_labels[k1k2[1]]\n",
    "\n",
    "            if k1k2[0] >1:\n",
    "                centroids_k, labels_centroids_based = calculate_centroids_and_predict_validation_data(training_set,train_labels_k,validation_set)\n",
    "\n",
    "            else:\n",
    "                labels_centroids_based = np.zeros(len(validation_set))\n",
    "\n",
    "            unique_valid_labels = list(range(k1k2[1]))\n",
    "            unique_centroid_labels = list(range(k1k2[0]))\n",
    "\n",
    "            true_positives, false_positives, true_negatives, false_negatives = get_confusion_matrix(valid_labels_k,labels_centroids_based,unique_valid_labels, unique_centroid_labels)\n",
    "            precision = get_precision(true_positives,false_positives)\n",
    "            recall = get_recall(true_positives, false_negatives)\n",
    "            f1 = get_F1_score(recall, precision)\n",
    "\n",
    "            F1_score_per_k_combination[k1k2] = f1\n",
    "\n",
    "        return F1_score_per_k_combination\n",
    "    \n",
    "    elif combination_type == \"own\":\n",
    "        print(\"Calculate F1 score based on training centroids with provided combination of possible clusters in both sets!\")\n",
    "        F1_score_per_k_combination = {} \n",
    "        for k1k2 in own_combinations:\n",
    "            train_labels_k = train_labels[k1k2[0]]\n",
    "            valid_labels_k = valid_labels[k1k2[1]]\n",
    "\n",
    "            if k1k2[0] >1:\n",
    "                centroids_k, labels_centroids_based = calculate_centroids_and_predict_validation_data(training_set,train_labels_k,validation_set)\n",
    "\n",
    "            else:\n",
    "                labels_centroids_based = np.zeros(len(validation_set))\n",
    "\n",
    "            unique_valid_labels = list(range(k1k2[1]))\n",
    "            unique_centroid_labels = list(range(k1k2[0]))\n",
    "\n",
    "            true_positives, false_positives, true_negatives, false_negatives = get_confusion_matrix(valid_labels_k,labels_centroids_based,unique_valid_labels, unique_centroid_labels)\n",
    "            precision = get_precision(true_positives,false_positives)\n",
    "            recall = get_recall(true_positives, false_negatives)\n",
    "            f1 = get_F1_score(recall, precision)\n",
    "\n",
    "            F1_score_per_k_combination[k1k2] = f1\n",
    "\n",
    "        return F1_score_per_k_combination       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "reg = None\n",
    "\n",
    "clear_prediction_strength_dir = \"Toy_data/Clearly_Separated/Prediction_Strength/\"\n",
    "ambig_prediction_strength_dir = \"Toy_data/Ambiguous/Ambiguous_Tau_Amplitude/Prediction_Strength/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels\n",
    "### balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_training_set_labels = np.load(clear_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "clear_validation_set_labels = np.load(clear_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "ambig_training_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_training.npy\" % (k, str(reg)))\n",
    "ambig_validation_set_labels = np.load(ambig_prediction_strength_dir + \"Labels/labels_k=%d_reg=%s_validation.npy\" % (k, str(reg)))\n",
    "\n",
    "\n",
    "clear_train_labels = {}\n",
    "clear_valid_labels = {}\n",
    "for i, labels in enumerate(clear_training_set_labels):\n",
    "    clear_train_labels[i+1] = labels\n",
    "    clear_valid_labels[i+1] = clear_validation_set_labels[i]\n",
    "    \n",
    "ambig_train_labels = {}\n",
    "ambig_valid_labels = {}\n",
    "for i, labels in enumerate(ambig_training_set_labels):\n",
    "    ambig_train_labels[i+1] = labels\n",
    "    ambig_valid_labels[i+1] = ambig_validation_set_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4], dtype=int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_validation_set_labels[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_training_set_labels = np.load(\"labels_k=%d_reg=%s_clear_training_unbalanced.npy\" % (k, str(reg)))\n",
    "clear_validation_set_labels = np.load(\"labels_k=%d_reg=%s_clear_validation_unbalanced.npy\" % (k, str(reg)))\n",
    "\n",
    "#ambig_training_set_labels = np.load(\"labels_k=%d_reg=%s_ambig_training_unbalanced.npy\" % (k, str(reg)))\n",
    "#ambig_validation_set_labels = np.load(\"labels_k=%d_reg=%s_ambig_validation_unbalanced.npy\" % (k, str(reg)))\n",
    "\n",
    "clear_train_labels = {}\n",
    "clear_valid_labels = {}\n",
    "for i, labels in enumerate(clear_training_set_labels):\n",
    "    clear_train_labels[i+1] = labels\n",
    "    clear_valid_labels[i+1] = clear_validation_set_labels[i]\n",
    "    \n",
    "#ambig_train_labels = {}\n",
    "#ambig_valid_labels = {}\n",
    "#for i, labels in enumerate(ambig_training_set_labels):\n",
    "#    ambig_train_labels[i+1] = labels\n",
    "#    ambig_valid_labels[i+1] = ambig_validation_set_labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score & Prediction Strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "clear_F1_score_per_k = get_F1_score_per_k(clear_data, clear_train_fold_indices[0], clear_train_fold_indices[1], clear_train_labels, clear_valid_labels, combination_type = \"true\" ,true_train_labels = clear_true_labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Predictions Strength per Cluster for each Clustering!\n"
     ]
    }
   ],
   "source": [
    "clear_prediction_strengths_per_k,_ = prediction_strength.get_prediction_strength_per_k(clear_data, clear_train_fold_indices[0], clear_train_fold_indices[1], clear_train_labels, clear_valid_labels, per_sample = False, true_train_labels = clear_true_labels_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate F1 score based on true training centroids!\n"
     ]
    }
   ],
   "source": [
    "ambig_F1_score_per_k = get_F1_score_per_k(ambig_data, ambig_train_fold_indices[0], ambig_train_fold_indices[1], ambig_train_labels, ambig_valid_labels,combination_type = \"true\" ,true_train_labels = ambig_true_labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Predictions Strength per Cluster for each Clustering!\n"
     ]
    }
   ],
   "source": [
    "ambig_prediction_strengths_per_k,_ = prediction_strength.get_prediction_strength_per_k(ambig_data, ambig_train_fold_indices[0], ambig_train_fold_indices[1], ambig_train_labels, ambig_valid_labels, per_sample = False, true_train_labels = ambig_true_labels_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Mean Prediction Strength vs. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20,10)\n",
    "plot_adjustments = [0.05,0.08,0.95, 0.91]\n",
    "configuration = \"k=%d - reg=%s\" % (k,str(reg))\n",
    "save_file = \"PS_vs_F1_k=%d_reg=%s_ambig_balanced_true.pdf\" % (k,str(reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_strengths_per_k = ambig_prediction_strengths_per_k\n",
    "F1_score_per_k = ambig_F1_score_per_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "k_clusters = list(prediction_strengths_per_k.keys())\n",
    "\n",
    "mean_prediction_strengths = []\n",
    "err_prediction_strengths = []\n",
    "min_prediction_strengths = []\n",
    "F1_scores = []\n",
    "\n",
    "for k in k_clusters:\n",
    "    mean_prediction_strengths.append(np.mean(prediction_strengths_per_k[k]))\n",
    "    err_prediction_strengths.append(np.std(prediction_strengths_per_k[k]))\n",
    "    min_prediction_strengths.append(np.amin(prediction_strengths_per_k[k]))\n",
    "    F1_scores.append(F1_score_per_k[k])\n",
    "\n",
    "\n",
    "upper_err = np.asarray(err_prediction_strengths) - np.maximum(0,(np.asarray(err_prediction_strengths)+np.asarray(mean_prediction_strengths)-1))\n",
    "lower_err = np.asarray(err_prediction_strengths)\n",
    "err = np.stack((lower_err,upper_err), axis=0)\n",
    "\n",
    "ax.plot(k_clusters, F1_scores, \"o-\", label=\"F1-Scores\",color = \"C03\",linewidth=3)\n",
    "ax.plot(k_clusters, mean_prediction_strengths, \"o-\",label=\"Mean PS\",color = \"C0\",linewidth=3)\n",
    "ax.plot(k_clusters, min_prediction_strengths, \"o-\", color = \"C01\", label=\"Min PS\",linewidth=3)\n",
    "\n",
    "argmax_f1 = np.argmax(F1_scores[1:]) + 1\n",
    "argmax_mean_ps = np.argmax(mean_prediction_strengths[1:]) +1\n",
    "argmax_min_ps = np.argmax(min_prediction_strengths[1:])+1\n",
    "\n",
    "\n",
    "if np.abs(argmax_mean_ps-argmax_f1) <8:\n",
    "    if F1_scores[argmax_f1] <= mean_prediction_strengths[argmax_mean_ps]:\n",
    "        ps_shift = 0.03\n",
    "        f1_shift = 0\n",
    "    else:\n",
    "        ps_shift = 0\n",
    "        f1_shift = 0.03\n",
    "else:\n",
    "    ps_shift = 0\n",
    "    f1_shift = 0\n",
    "\n",
    "ax.annotate(\"#%d|Score=%.3f\" % (argmax_mean_ps+1, mean_prediction_strengths[argmax_mean_ps]), (k_clusters[argmax_mean_ps] - 1, mean_prediction_strengths[argmax_mean_ps] + 0.03 + ps_shift),fontsize = 16, color = \"C0\")\n",
    "ax.annotate(\"#%d|Score=%.3f\" % (argmax_f1+1, F1_scores[argmax_f1]), (k_clusters[argmax_f1] - 1, F1_scores[argmax_f1] + 0.03 + f1_shift), fontsize=16, color = \"C03\")\n",
    "ax.annotate(\"#%d|Score=%.3f\" % (argmax_min_ps+1, min_prediction_strengths[argmax_min_ps]), (k_clusters[argmax_min_ps] - 1, min_prediction_strengths[argmax_min_ps] + 0.03), fontsize=16, color = \"C01\")\n",
    "\n",
    "title = \"Prediction Strength vs. F1-Score for Clustering with k Clusters \\n\" + configuration \n",
    "\n",
    "ax.set_title(title, fontsize=22, pad=20)\n",
    "ax.set_xticks(k_clusters)\n",
    "ax.set_xlabel(\"# Number of clusters\", fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"Score\", fontsize=18, labelpad=10),\n",
    "ax.set_ylim((0, 1.1))\n",
    "ax.tick_params(axis='y',labelsize=14)\n",
    "ax.tick_params(axis='x',labelsize=14)\n",
    "\n",
    "ax.set_yticks(np.arange(0, 1.1,0.1))\n",
    "left = plot_adjustments[0]\n",
    "bottom = plot_adjustments[1]\n",
    "right = plot_adjustments[2]\n",
    "top = plot_adjustments[3]\n",
    "\n",
    "plt.subplots_adjust(left,bottom,right, top)\n",
    "\n",
    "ax.legend(fontsize = 14, loc=\"lower right\")\n",
    "\n",
    "plt.savefig(save_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "reg = 10\n",
    "k_clusters = 35\n",
    "save_file_clusters = \"PS_vs_F1_clusters_k=%d_reg=%s_clear_balanced_true_kclusters=%d_validation.pdf\" % (k,str(reg),k_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:335: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n"
     ]
    }
   ],
   "source": [
    "functions_for_plotting.plot_clusters(clear_validation_set, clear_true_labels[clear_train_fold_indices[1]],clear_valid_labels[k_clusters], 3,4, clear_layout_label_mapping,figsize=(20,20),n_bursts = 100,y_lim = (0,16),save_file=save_file_clusters ,subplot_adjustments= [0.05,0.95,0.03,0.9,0.4, 0.15], plot_mean=False, title= \"Validation Set Clusters \\n k=%d, $\\lambda$=%s\" % (k,str(reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "reg = None\n",
    "k_clusters = 49\n",
    "save_file_clusters = \"PS_vs_F1_clusters_k=%d_reg=%s_clear_balanced_true_kclusters=%d.pdf\" % (k,str(reg),k_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:271: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:277: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_start:row_end, corresponding_column])\n",
      "/Users/paule/Desktop/Burst_Clustering/functions_for_plotting.py:335: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(outer_grid[row_i:(row_i + 1), column_i])\n"
     ]
    }
   ],
   "source": [
    "functions_for_plotting.plot_clusters(ambig_validation_set, ambig_true_labels[ambig_train_fold_indices[1]],ambig_valid_labels[k_clusters], 10,5, ambig_layout_label_mapping,figsize=(40,30),n_bursts = 100,y_lim = (0,16),save_file=save_file_clusters ,subplot_adjustments= [0.05,0.93,0.02,0.92,0.9, 0.2], plot_mean=False, title= \"Validation Set Clusters \\n k=%d, $\\lambda$=%s\" % (k,str(reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_combinations = list(product([8,9,10,11,12,13,14], repeat=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score_per_k = get_F1_score_per_k(data, train_fold_indices[0], train_fold_indices[1], train_labels, valid_labels,combination_type = \"own\" ,own_combinations = own_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 8): 0.6398728741897797,\n",
       " (8, 9): 0.6523157422484107,\n",
       " (8, 10): 0.747866441051456,\n",
       " (8, 11): 0.7676779124301703,\n",
       " (8, 12): 0.7643628278290526,\n",
       " (8, 13): 0.7561196701895473,\n",
       " (8, 14): 0.7517187506416196,\n",
       " (9, 8): 0.6206285457021343,\n",
       " (9, 9): 0.6348035795466058,\n",
       " (9, 10): 0.54128442367322,\n",
       " (9, 11): 0.561106675340261,\n",
       " (9, 12): 0.5566062570062832,\n",
       " (9, 13): 0.5430555708250097,\n",
       " (9, 14): 0.5369686305500886,\n",
       " (10, 8): 0.6586809967770928,\n",
       " (10, 9): 0.6740591614866355,\n",
       " (10, 10): 0.8230800217314856,\n",
       " (10, 11): 0.8504598894390333,\n",
       " (10, 12): 0.8465166780851942,\n",
       " (10, 13): 0.8306834166226942,\n",
       " (10, 14): 0.825293948204031,\n",
       " (11, 8): 0.6611515311436493,\n",
       " (11, 9): 0.6769921408954339,\n",
       " (11, 10): 0.8286083600749147,\n",
       " (11, 11): 0.856079256417438,\n",
       " (11, 12): 0.8542812373582425,\n",
       " (11, 13): 0.8382913727952211,\n",
       " (11, 14): 0.8328617080469184,\n",
       " (12, 8): 0.5921517504884093,\n",
       " (12, 9): 0.607038716831298,\n",
       " (12, 10): 0.7748125031675737,\n",
       " (12, 11): 0.8052822141267895,\n",
       " (12, 12): 0.8032479813523717,\n",
       " (12, 13): 0.7814021304738048,\n",
       " (12, 14): 0.7753887565363575,\n",
       " (13, 8): 0.5959305003952147,\n",
       " (13, 9): 0.6111247368242722,\n",
       " (13, 10): 0.797562493812477,\n",
       " (13, 11): 0.829356460632175,\n",
       " (13, 12): 0.8265452460095191,\n",
       " (13, 13): 0.8568527626991838,\n",
       " (13, 14): 0.851022068341163,\n",
       " (14, 8): 0.5937006777411005,\n",
       " (14, 9): 0.6082966550460939,\n",
       " (14, 10): 0.7950570396707787,\n",
       " (14, 11): 0.8260682846067443,\n",
       " (14, 12): 0.8274486466799679,\n",
       " (14, 13): 0.8578706265883199,\n",
       " (14, 14): 0.8520278451524332}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score_per_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = np.argmax(list(F1_score_per_k.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = np.asarray(list(F1_score_per_k.keys()))[argmax]\n",
    "f1_score = np.asarray(list(F1_score_per_k.values()))[argmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31849359707172603"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score_per_k[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6195245612672132"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
